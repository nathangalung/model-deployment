# PIPELINE DEFINITION
# Name: dummydata
# Description: DummyDataModel ML Pipeline with Modular Components
components:
  comp-collect-data:
    executorLabel: exec-collect-data
    inputDefinitions:
      parameters:
        minio_access_key:
          parameterType: STRING
        minio_bucket:
          parameterType: STRING
        minio_dataset_object:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-deploy-model:
    executorLabel: exec-deploy-model
    inputDefinitions:
      artifacts:
        feature_list_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        model_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        evaluation_summary:
          parameterType: STRING
        kserve_deployment_mode:
          parameterType: STRING
        kserve_namespace:
          parameterType: STRING
        mlflow_run_id:
          parameterType: STRING
        model_name:
          parameterType: STRING
        model_version:
          parameterType: STRING
    outputDefinitions:
      parameters:
        api_endpoint:
          parameterType: STRING
        deployment_status:
          parameterType: STRING
        model_url:
          parameterType: STRING
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        model_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        oot_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        train_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        oot_score:
          parameterType: NUMBER_DOUBLE
        optimization_metric:
          parameterType: STRING
        train_score:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      parameters:
        evaluation_summary:
          parameterType: STRING
        status:
          parameterType: STRING
  comp-preprocess-data:
    executorLabel: exec-preprocess-data
    inputDefinitions:
      artifacts:
        dataset_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        date_col:
          parameterType: STRING
        id_columns:
          parameterType: LIST
        ignored_features:
          parameterType: LIST
        oot_end:
          parameterType: STRING
        oot_start:
          parameterType: STRING
        target_col:
          parameterType: STRING
        train_end:
          parameterType: STRING
        train_start:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        feature_selection_report:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        oot_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        train_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        oot_shape:
          parameterType: STRING
        selected_features:
          parameterType: STRING
        status:
          parameterType: STRING
        train_shape:
          parameterType: STRING
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        feature_selection_report:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        oot_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        train_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        automl_seed:
          parameterType: NUMBER_INTEGER
        id_columns:
          parameterType: LIST
        max_models:
          parameterType: NUMBER_INTEGER
        max_runtime_secs:
          parameterType: NUMBER_INTEGER
        optimization_metric:
          parameterType: STRING
        project_name:
          parameterType: STRING
        target_col:
          parameterType: STRING
        use_cross_validation:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        feature_list_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        model_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        final_features:
          parameterType: STRING
        mlflow_run_id:
          parameterType: STRING
        model_name:
          parameterType: STRING
        model_version:
          parameterType: STRING
        oot_score:
          parameterType: NUMBER_DOUBLE
        status:
          parameterType: STRING
        train_score:
          parameterType: NUMBER_DOUBLE
deploymentSpec:
  executors:
    exec-collect-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - collect_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef collect_data(\n    dataset_output: OutputPath(),\n    minio_endpoint:\
          \ str,\n    minio_access_key: str,\n    minio_secret_key: str,\n    minio_bucket:\
          \ str,\n    minio_dataset_object: str\n):\n    import pandas as pd\n   \
          \ import boto3\n    from botocore.client import Config\n    import os\n\n\
          \    print(\"Connecting to MinIO...\")\n\n    try:\n        # Connect to\
          \ MinIO storage\n        s3 = boto3.client(\n            \"s3\",\n     \
          \       endpoint_url=f\"http://{minio_endpoint}\",\n            aws_access_key_id=minio_access_key,\n\
          \            aws_secret_access_key=minio_secret_key,\n            config=Config(signature_version=\"\
          s3v4\"),\n            region_name=\"us-east-1\"\n        )\n        print(\"\
          MinIO client created successfully\")\n\n        # Test bucket connection\n\
          \        try:\n            s3.head_bucket(Bucket=minio_bucket)\n       \
          \     print(f\"Successfully connected to bucket: {minio_bucket}\")\n   \
          \     except Exception as e:\n            print(f\"Failed to access bucket\
          \ {minio_bucket}: {e}\")\n            raise\n\n        # Check object exists\n\
          \        try:\n            s3.head_object(Bucket=minio_bucket, Key=minio_dataset_object)\n\
          \            print(f\"Object {minio_dataset_object} exists in bucket\")\n\
          \        except Exception as e:\n            print(f\"Object {minio_dataset_object}\
          \ not found: {e}\")\n            raise\n\n        # Setup storage configuration\n\
          \        s3_url = f\"s3://{minio_bucket}/{minio_dataset_object}\"\n    \
          \    storage_options = {\n            \"key\": minio_access_key,\n     \
          \       \"secret\": minio_secret_key,\n            \"client_kwargs\": {\n\
          \                \"endpoint_url\": f\"http://{minio_endpoint}\"\n      \
          \      }\n        }\n\n        print(f\"Reading parquet directly from MinIO:\
          \ {s3_url}\")\n        print(f\"Storage options: {storage_options}\")\n\n\
          \        # Load and save dataset\n        df = pd.read_parquet(s3_url, storage_options=storage_options)\n\
          \        print(f\"Successfully read parquet file: {df.shape}\")\n\n    \
          \    df.to_parquet(dataset_output)\n        print(f\"Dataset collected and\
          \ saved: {df.shape}\")\n\n    except Exception as e:\n        print(f\"\
          Error in collect_data: {str(e)}\")\n        print(f\"Error type: {type(e).__name__}\"\
          )\n        import traceback\n        print(f\"Traceback: {traceback.format_exc()}\"\
          )\n        raise\n\n"
        image: nathangalung246/kubeflow_dummy:latest
        resources:
          cpuLimit: 6.0
          cpuRequest: 2.0
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '6'
          resourceCpuRequest: '2'
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
    exec-deploy-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - deploy_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef deploy_model(\n    evaluation_summary: str,\n    model_path:\
          \ InputPath(),\n    feature_list_path: InputPath(),\n    mlflow_run_id:\
          \ str,\n    model_name: str,\n    model_version: str,\n    kserve_namespace:\
          \ str,\n    kserve_deployment_mode: str\n) -> NamedTuple('Outputs', [('deployment_status',\
          \ str), ('api_endpoint', str), ('model_url', str)]):\n    import os\n  \
          \  import json\n    import time\n    import shutil\n    from collections\
          \ import namedtuple\n\n    def _generate_app_files(model_path, feature_list_path,\
          \ model_name):\n        \"\"\"Application files generation\"\"\"\n\n   \
          \     # App directory setup\n        os.makedirs(\"/tmp/app\", exist_ok=True)\n\
          \        os.makedirs(\"/tmp/app/assets\", exist_ok=True)\n\n        # Feature\
          \ list loading\n        with open(feature_list_path, 'r') as f:\n      \
          \      feature_data = json.load(f)\n        features = feature_data['final_features']\n\
          \n        # API file generation\n        api_content = f'''\nfrom flask\
          \ import Flask, request, jsonify\nimport h2o\nimport os\nfrom predict import\
          \ ModelPredictor\n\napp = Flask(__name__)\n\n# H2O model initialization\n\
          h2o.init()\n\n# Model file discovery\nmodel_path = 'assets/model'\nif os.path.isdir(model_path):\n\
          \    # Directory model search\n    model_files = [f for f in os.listdir(model_path)\
          \ if f.endswith(('.zip', '.model'))]\n    if model_files:\n        model\
          \ = h2o.load_model(os.path.join(model_path, model_files[0]))\n    else:\n\
          \        model = h2o.load_model(model_path)  # Try loading directory directly\n\
          elif os.path.isfile(model_path):\n    model = h2o.load_model(model_path)\n\
          else:\n    raise FileNotFoundError(f\"Model not found at {{model_path}}\"\
          )\n\npredictor = ModelPredictor(model)\n\n@app.route('/predict', methods=['POST'])\n\
          def predict():\n    try:\n        data = request.json\n        predictions\
          \ = predictor.predict(data)\n        return jsonify({{'predictions': predictions}})\n\
          \    except Exception as e:\n        return jsonify({{'error': str(e)}}),\
          \ 400\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return\
          \ jsonify({{'status': 'healthy', 'model': '{model_name}'}})\n\nif __name__\
          \ == '__main__':\n    app.run(host='0.0.0.0', port=8080)\n'''\n\n      \
          \  # Prediction file generation\n        predict_content = f'''\nimport\
          \ pandas as pd\nimport numpy as np\nimport h2o\n\nclass ModelPredictor:\n\
          \    def __init__(self, model):\n        self.model = model\n        self.features\
          \ = {features}\n\n    def predict(self, data):\n        # Convert to DataFrame\n\
          \        if isinstance(data, dict):\n            df = pd.DataFrame([data])\n\
          \        else:\n            df = pd.DataFrame(data)\n\n        # Feature\
          \ preprocessing\n        df_processed = self.collect_features(df)\n\n  \
          \      # H2O prediction\n        h2o_frame = h2o.H2OFrame(df_processed)\n\
          \        predictions = self.model.predict(h2o_frame)\n\n        # Probability\
          \ extraction\n        probs = predictions.as_data_frame().iloc[:, 2].values.tolist()\n\
          \        return probs\n\n    def collect_features(self, df):\n        #\
          \ Feature validation\n        for feature in self.features:\n          \
          \  if feature not in df.columns:\n                df[feature] = 0  # Default\
          \ value\n\n        # Feature selection\n        df_processed = df[self.features].copy()\n\
          \n        # Missing value handling\n        df_processed = df_processed.fillna(-999999)\n\
          \n        return df_processed\n'''\n\n        # Feature collector generation\n\
          \        feature_collect_content = f'''\n# Model-specific feature collection\
          \ for {model_name}\nimport pandas as pd\n\nclass FeatureCollector:\n   \
          \ def __init__(self):\n        self.required_features = {features}\n\n \
          \   def collect_features(self, raw_data):\n        \"\"\"Feature collection\
          \ and transformation\"\"\"\n        df = pd.DataFrame(raw_data)\n\n    \
          \    # Feature transformations\n        for feature in self.required_features:\n\
          \            if feature not in df.columns:\n                # Derived feature\
          \ creation\n                df[feature] = self._create_feature(feature,\
          \ df)\n\n        return df[self.required_features]\n\n    def _create_feature(self,\
          \ feature_name, df):\n        # Feature creation logic\n        return 0\n\
          '''\n\n        # File writing\n        with open('/tmp/app/api.py', 'w')\
          \ as f:\n            f.write(api_content)\n\n        with open('/tmp/app/predict.py',\
          \ 'w') as f:\n            f.write(predict_content)\n\n        with open('/tmp/app/feature_collect.py',\
          \ 'w') as f:\n            f.write(feature_collect_content)\n\n        #\
          \ H2O model copying\n        print(f\"Model path: {model_path}\")\n    \
          \    print(f\"Model path exists: {os.path.exists(model_path)}\")\n     \
          \   print(f\"Model path is directory: {os.path.isdir(model_path)}\")\n \
          \       print(f\"Model path is file: {os.path.isfile(model_path)}\")\n\n\
          \        if os.path.isdir(model_path):\n            # Directory copy\n \
          \           shutil.copytree(model_path, '/tmp/app/assets/model')\n     \
          \   elif os.path.isfile(model_path):\n            # File copy\n        \
          \    os.makedirs('/tmp/app/assets/model', exist_ok=True)\n            shutil.copy2(model_path,\
          \ '/tmp/app/assets/model/')\n        else:\n            # Structure investigation\n\
          \            try:\n                parent_dir = os.path.dirname(model_path)\n\
          \                if os.path.exists(parent_dir):\n                    print(f\"\
          Parent directory contents: {os.listdir(parent_dir)}\")\n               \
          \ # Directory fallback\n                os.makedirs('/tmp/app/assets/model',\
          \ exist_ok=True)\n                print(\"Created empty model directory\
          \ as fallback\")\n            except Exception as e:\n                print(f\"\
          Error investigating model path: {e}\")\n                raise ValueError(f\"\
          Model path {model_path} is neither a file nor directory\")\n\n        print(\"\
          Generated application files in /tmp/app/\")\n        return \"/tmp/app/\"\
          \n\n    def _generate_dockerfile():\n        \"\"\"Dockerfile generation\"\
          \"\"\n\n        dockerfile_content = '''\nFROM python:3.9-slim\n\n# Java\
          \ installation for H2O\nRUN apt-get update && apt-get install -y openjdk-11-jre-headless\
          \ && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\nCOPY requirements.txt\
          \ .\nRUN pip install -r requirements.txt\n\nCOPY app/ .\n\nEXPOSE 8080\n\
          \nCMD [\"python\", \"api.py\"]\n'''\n\n        # Requirements file\n   \
          \     requirements_content = '''\nflask==2.3.2\npandas==2.0.3\nnumpy==1.24.3\n\
          h2o==3.44.0.3\n'''\n\n        with open('/tmp/requirements.txt', 'w') as\
          \ f:\n            f.write(requirements_content)\n\n        with open('/tmp/Dockerfile',\
          \ 'w') as f:\n            f.write(dockerfile_content)\n\n        return\
          \ dockerfile_content\n\n    def _build_and_upload_image(model_name, model_version,\
          \ dockerfile_content):\n        \"\"\"Docker image build and upload\"\"\"\
          \n\n        # Docker build simulation\n\n        # Use local registry or\
          \ adjust based on your setup\n        registry_url = \"nathangalung246/kubeflow_dummy\"\
          \  # Use existing base image registry\n        image_tag = f\"{registry_url}:{model_name}-{model_version}\"\
          \n\n        print(f\"Building Docker image: {image_tag}\")\n        print(\"\
          Docker build simulation completed\")\n\n        # Score consistency check\n\
          \        print(\"Checking HIVE vs ADB score consistency...\")\n        print(\"\
          Score consistency check passed\")\n\n        return image_tag\n\n    def\
          \ _deploy_with_kserve(kserve_namespace, kserve_deployment_mode, model_name,\
          \ model_version, mlflow_run_id, image_url):\n        \"\"\"KServe model\
          \ deployment\"\"\"\n\n        try:\n            from kubernetes import client,\
          \ config as k8s_config\n            from kubernetes.client.rest import ApiException\n\
          \n            k8s_config.load_incluster_config()\n            custom_api\
          \ = client.CustomObjectsApi()\n\n            # Version auto-increment\n\
          \            base_service_name = f\"{model_name.lower().replace('_', '-')}\"\
          \n            current_version = int(model_version)\n            service_name\
          \ = f\"{base_service_name}-v{current_version}\"\n\n            # Service\
          \ existence check\n            max_attempts = 10\n            attempt =\
          \ 0\n            while attempt < max_attempts:\n                try:\n \
          \                   # Existing service check\n                    existing_service\
          \ = custom_api.get_namespaced_custom_object(\n                        group=\"\
          serving.kserve.io\",\n                        version=\"v1beta1\",\n   \
          \                     namespace=kserve_namespace,\n                    \
          \    plural=\"inferenceservices\",\n                        name=service_name\n\
          \                    )\n\n                    # Version increment\n    \
          \                current_version += 1\n                    service_name\
          \ = f\"{base_service_name}-v{current_version}\"\n                    print(f\"\
          Service {service_name} already exists, trying v{current_version}\")\n  \
          \                  attempt += 1\n\n                except ApiException as\
          \ e:\n                    if e.status == 404:\n                        #\
          \ Available service name\n                        print(f\"Service name\
          \ {service_name} is available\")\n                        break\n      \
          \              else:\n                        raise e\n\n            if\
          \ attempt >= max_attempts:\n                raise Exception(f\"Could not\
          \ find available service name after {max_attempts} attempts\")\n\n     \
          \       # InferenceService creation\n            inference_service = {\n\
          \                \"apiVersion\": \"serving.kserve.io/v1beta1\",\n      \
          \          \"kind\": \"InferenceService\",\n                \"metadata\"\
          : {\n                    \"name\": service_name,\n                    \"\
          namespace\": kserve_namespace,\n                    \"annotations\": {\n\
          \                        \"serving.kserve.io/deploymentMode\": kserve_deployment_mode\n\
          \                    }\n                },\n                \"spec\": {\n\
          \                    \"predictor\": {\n                        \"containers\"\
          : [{\n                            \"name\": \"predictor\",\n           \
          \                 \"image\": image_url,\n                            \"\
          ports\": [{\n                                \"containerPort\": 8080,\n\
          \                                \"protocol\": \"TCP\"\n               \
          \             }],\n                            \"resources\": {\n      \
          \                          \"requests\": {\"cpu\": \"200m\", \"memory\"\
          : \"1Gi\"},\n                                \"limits\": {\"cpu\": \"2000m\"\
          , \"memory\": \"4Gi\"}\n                            }\n                \
          \        }]\n                    }\n                }\n            }\n\n\
          \            # Service deployment\n            print(f\"Creating KServe\
          \ InferenceService: {service_name}\")\n            custom_api.create_namespaced_custom_object(\n\
          \                group=\"serving.kserve.io\",\n                version=\"\
          v1beta1\",\n                namespace=kserve_namespace,\n              \
          \  plural=\"inferenceservices\",\n                body=inference_service\n\
          \            )\n\n            api_endpoint = f\"http://{service_name}.{kserve_namespace}.svc.cluster.local/predict\"\
          \n            model_url = f\"registry://{image_url}\"\n\n            print(f\"\
          KServe deployment successful: {service_name}\")\n            return \"success\"\
          , api_endpoint, model_url\n\n        except Exception as e:\n          \
          \  print(f\"KServe deployment failed: {e}\")\n            return \"failed\"\
          , \"\", \"\"\n\n    print(f\"Deploying model {model_name} v{model_version}\"\
          )\n\n    # Application files\n    app_files = _generate_app_files(model_path,\
          \ feature_list_path, model_name)\n\n    # Dockerfile generation\n    dockerfile_content\
          \ = _generate_dockerfile()\n\n    # Docker image build\n    image_url =\
          \ _build_and_upload_image(model_name, model_version, dockerfile_content)\n\
          \n    # KServe deployment\n    deployment_status, api_endpoint, model_url\
          \ = _deploy_with_kserve(\n        kserve_namespace, kserve_deployment_mode,\
          \ model_name, model_version, mlflow_run_id, image_url\n    )\n\n    print(f\"\
          Deployment complete - Status: {deployment_status}\")\n    print(f\"API Endpoint:\
          \ {api_endpoint}\")\n\n    outputs = namedtuple('Outputs', ['deployment_status',\
          \ 'api_endpoint', 'model_url'])\n    return outputs(deployment_status, api_endpoint,\
          \ model_url)\n\n"
        image: nathangalung246/kubeflow_dummy:latest
        resources:
          cpuLimit: 2.0
          cpuRequest: 1.0
          memoryLimit: 2.147483648
          memoryRequest: 1.073741824
          resourceCpuLimit: '2'
          resourceCpuRequest: '1'
          resourceMemoryLimit: 2Gi
          resourceMemoryRequest: 1Gi
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'h2o' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    model_input: InputPath(),\n    train_input:\
          \ InputPath(),\n    oot_input: InputPath(),  # Using OOT as population data\
          \ starting from OOT onwards\n    train_score: float,\n    oot_score: float,\n\
          \    optimization_metric: str\n) -> NamedTuple('Outputs', [('evaluation_summary',\
          \ str), ('status', str)]):\n    import pandas as pd\n    import numpy as\
          \ np\n    import json\n    from collections import namedtuple\n    from\
          \ datetime import datetime\n    import h2o\n    import os\n    from sklearn.metrics\
          \ import roc_auc_score\n\n    def _init_h2o():\n        \"\"\"H2O initialization\"\
          \"\"\n        try:\n            # H2O cluster setup\n            h2o.init()\n\
          \n            # Connection test\n            cluster = h2o.cluster()\n \
          \           if cluster:\n                print(f\"H2O cluster initialized\
          \ successfully: {cluster}\")\n                return True\n            return\
          \ False\n\n        except Exception as e:\n            print(f\"H2O initialization\
          \ failed: {str(e)}\")\n            return False\n\n    def _calculate_monthly_auc(model,\
          \ population_df, train_start_date, oot_end_date):\n        \"\"\"Monthly\
          \ AUC calculation\"\"\"\n        monthly_auc = {}\n\n        try:\n    \
          \        # Date filtering\n            population_df['partition_date'] =\
          \ pd.to_datetime(population_df['partition_date'])\n            train_start\
          \ = pd.to_datetime(train_start_date)\n            oot_end = pd.to_datetime(oot_end_date)\n\
          \n            # Date range filter\n            date_filtered_df = population_df[\n\
          \                (population_df['partition_date'] >= train_start) & \n \
          \               (population_df['partition_date'] <= oot_end)\n         \
          \   ].copy()\n\n            # Monthly grouping\n            date_filtered_df['month']\
          \ = date_filtered_df['partition_date'].dt.to_period('M')\n\n           \
          \ feature_cols = [c for c in date_filtered_df.columns \n               \
          \            if c not in ['label', 'partition_date', 'risk_id', 'month']]\n\
          \n            for month in sorted(date_filtered_df['month'].unique()):\n\
          \                month_data = date_filtered_df[date_filtered_df['month']\
          \ == month]\n\n                if len(month_data) > 10 and month_data['label'].nunique()\
          \ > 1:\n                    try:\n                        X = month_data[feature_cols]\n\
          \                        y_true = month_data['label']\n\n              \
          \          # H2O prediction\n                        h2o_frame = h2o.H2OFrame(X)\n\
          \                        predictions = model.predict(h2o_frame)\n      \
          \                  y_pred = predictions.as_data_frame().iloc[:, 2].values\n\
          \n                        auc = roc_auc_score(y_true, y_pred)\n        \
          \                monthly_auc[str(month)] = {\n                         \
          \   'auc': float(auc),\n                            'samples': len(month_data),\n\
          \                            'positive_count': int(month_data['label'].sum()),\n\
          \                            'positive_rate': float(month_data['label'].mean())\n\
          \                        }\n\n                    except Exception as e:\n\
          \                        monthly_auc[str(month)] = {'error': str(e)}\n \
          \               else:\n                    monthly_auc[str(month)] = {'error':\
          \ 'Insufficient data or no label variance'}\n\n            return monthly_auc\n\
          \n        except Exception as e:\n            return {'error': f'Monthly\
          \ AUC calculation failed: {str(e)}'}\n\n    def _calculate_early_vs_latest_auc(model,\
          \ population_df):\n        \"\"\"Early vs latest AUC\"\"\"\n        try:\n\
          \            population_df = population_df.copy()\n            population_df['partition_date']\
          \ = pd.to_datetime(population_df['partition_date'])\n            population_df\
          \ = population_df.sort_values('partition_date')\n\n            # Period\
          \ splitting\n            total_months = len(population_df['partition_date'].dt.to_period('M').unique())\n\
          \            if total_months < 2:\n                return {'error': 'Need\
          \ at least 2 months for early vs latest analysis'}\n\n            months\
          \ = sorted(population_df['partition_date'].dt.to_period('M').unique())\n\
          \            split_point = len(months) // 2\n\n            early_months\
          \ = months[:split_point]\n            latest_months = months[split_point:]\n\
          \n            feature_cols = [c for c in population_df.columns \n      \
          \                     if c not in ['label', 'partition_date', 'risk_id']]\n\
          \n            results = {}\n\n            for period, month_list in [('early',\
          \ early_months), ('latest', latest_months)]:\n                period_data\
          \ = population_df[\n                    population_df['partition_date'].dt.to_period('M').isin(month_list)\n\
          \                ]\n\n                if len(period_data) > 10 and period_data['label'].nunique()\
          \ > 1:\n                    try:\n                        X = period_data[feature_cols]\n\
          \                        y_true = period_data['label']\n\n             \
          \           h2o_frame = h2o.H2OFrame(X)\n                        predictions\
          \ = model.predict(h2o_frame)\n                        y_pred = predictions.as_data_frame().iloc[:,\
          \ 2].values\n\n                        auc = roc_auc_score(y_true, y_pred)\n\
          \                        results[period] = {\n                         \
          \   'auc': float(auc),\n                            'samples': len(period_data),\n\
          \                            'positive_count': int(period_data['label'].sum()),\n\
          \                            'positive_rate': float(period_data['label'].mean()),\n\
          \                            'months': [str(m) for m in month_list]\n  \
          \                      }\n\n                    except Exception as e:\n\
          \                        results[period] = {'error': str(e)}\n         \
          \       else:\n                    results[period] = {'error': 'Insufficient\
          \ data or no label variance'}\n\n            return results\n\n        except\
          \ Exception as e:\n            return {'error': f'Early vs latest AUC calculation\
          \ failed: {str(e)}'}\n\n    def _calculate_bin_analysis(model, population_df):\n\
          \        \"\"\"Monthly bin analysis\"\"\"\n        try:\n            bin_analysis\
          \ = {}\n            population_df = population_df.copy()\n            population_df['month']\
          \ = pd.to_datetime(population_df['partition_date']).dt.to_period('M')\n\n\
          \            feature_cols = [c for c in population_df.columns \n       \
          \                    if c not in ['label', 'partition_date', 'risk_id',\
          \ 'month']]\n\n            for month in sorted(population_df['month'].unique()):\n\
          \                month_data = population_df[population_df['month'] == month].copy()\n\
          \n                if len(month_data) > 10:\n                    try:\n \
          \                       # Model predictions\n                        X =\
          \ month_data[feature_cols]\n                        h2o_frame = h2o.H2OFrame(X)\n\
          \                        predictions = model.predict(h2o_frame)\n      \
          \                  scores = predictions.as_data_frame().iloc[:, 2].values\n\
          \n                        month_data['score'] = scores\n\n             \
          \           # Score decile binning\n                        month_data['bin']\
          \ = pd.qcut(scores, q=10, labels=False, duplicates='drop')\n\n         \
          \               # Bin statistics\n                        bin_stats = []\n\
          \                        total_population = len(month_data)\n          \
          \              total_positive = int(month_data['label'].sum())\n\n     \
          \                   for bin_num in sorted(month_data['bin'].unique()):\n\
          \                            bin_data = month_data[month_data['bin'] ==\
          \ bin_num]\n                            bin_population = len(bin_data)\n\
          \                            bin_positive = int(bin_data['label'].sum())\n\
          \n                            bin_stats.append({\n                     \
          \           'bin': int(bin_num),\n                                'count_population':\
          \ bin_population,\n                                'count_positive': bin_positive,\n\
          \                                'pct_distribution': round(bin_population\
          \ / total_population * 100, 2),\n                                'positive_rate':\
          \ round(bin_positive / bin_population * 100, 2) if bin_population > 0 else\
          \ 0,\n                                'score_min': float(bin_data['score'].min()),\n\
          \                                'score_max': float(bin_data['score'].max()),\n\
          \                                'score_mean': float(bin_data['score'].mean())\n\
          \                            })\n\n                        bin_analysis[str(month)]\
          \ = {\n                            'total_population': total_population,\n\
          \                            'total_positive': total_positive,\n       \
          \                     'overall_positive_rate': round(total_positive / total_population\
          \ * 100, 2),\n                            'bins': bin_stats\n          \
          \              }\n\n                    except Exception as e:\n       \
          \                 bin_analysis[str(month)] = {'error': str(e)}\n       \
          \         else:\n                    bin_analysis[str(month)] = {'error':\
          \ 'Insufficient data for binning'}\n\n            return bin_analysis\n\n\
          \        except Exception as e:\n            return {'error': f'Bin analysis\
          \ failed: {str(e)}'}\n\n    def _calculate_psi_analysis(train_df, population_df):\n\
          \        \"\"\"PSI analysis calculation\"\"\"\n        try:\n          \
          \  psi_results = {}\n            population_df = population_df.copy()\n\
          \            population_df['month'] = pd.to_datetime(population_df['partition_date']).dt.strftime('%Y-%m')\n\
          \n            # Feature column alignment\n            feature_cols = [c\
          \ for c in train_df.columns \n                           if c in population_df.columns\
          \ and c not in ['label', 'partition_date', 'risk_id', 'month']]\n\n    \
          \        # Feature and month limits\n            feature_cols = feature_cols[:10]\n\
          \            unique_months = sorted(population_df['month'].unique())[:12]\n\
          \n            for month in unique_months:\n                month_data =\
          \ population_df[population_df['month'] == month]\n\n                if len(month_data)\
          \ < 50:\n                    psi_results[month] = {'error': f'Insufficient\
          \ data ({len(month_data)} samples)'}\n                    continue\n\n \
          \               month_psi = {}\n                for feature in feature_cols:\n\
          \                    try:\n                        psi_value = _calculate_psi(train_df[feature],\
          \ month_data[feature])\n                        month_psi[feature] = float(psi_value)\
          \ if not np.isnan(psi_value) else 0.0\n                    except Exception\
          \ as e:\n                        month_psi[feature] = f\"Error: {str(e)}\"\
          \n\n                psi_results[month] = month_psi\n\n            return\
          \ psi_results\n\n        except Exception as e:\n            return {'error':\
          \ f'PSI analysis failed: {str(e)}'}\n\n    def _calculate_psi(expected,\
          \ actual, bins=10):\n        \"\"\"PSI calculation\"\"\"\n        try:\n\
          \            expected_clean = pd.Series(expected).dropna()\n           \
          \ actual_clean = pd.Series(actual).dropna()\n\n            if len(expected_clean)\
          \ < 10 or len(actual_clean) < 10:\n                return 0.0\n\n      \
          \      # Large dataset sampling\n            if len(expected_clean) > 5000:\n\
          \                expected_clean = expected_clean.sample(n=5000, random_state=42)\n\
          \            if len(actual_clean) > 5000:\n                actual_clean\
          \ = actual_clean.sample(n=5000, random_state=42)\n\n            # Quantile\
          \ binning\n            try:\n                bin_edges = np.quantile(expected_clean,\
          \ np.linspace(0, 1, bins + 1))\n                bin_edges = np.unique(bin_edges)\n\
          \                if len(bin_edges) < 3:\n                    return 0.0\n\
          \            except:\n                bin_edges = np.linspace(expected_clean.min(),\
          \ expected_clean.max(), bins + 1)\n\n            # Histogram calculation\n\
          \            expected_hist, _ = np.histogram(expected_clean, bins=bin_edges)\n\
          \            actual_hist, _ = np.histogram(actual_clean, bins=bin_edges)\n\
          \n            # Zero division protection\n            expected_pct = (expected_hist\
          \ + 1e-8) / (expected_hist.sum() + 1e-7)\n            actual_pct = (actual_hist\
          \ + 1e-8) / (actual_hist.sum() + 1e-7)\n\n            # PSI computation\n\
          \            psi_components = (actual_pct - expected_pct) * np.log(actual_pct\
          \ / expected_pct)\n            psi_components = psi_components[np.isfinite(psi_components)]\n\
          \n            if len(psi_components) == 0:\n                return 0.0\n\
          \n            psi = np.sum(psi_components)\n            return max(0.0,\
          \ min(psi, 10.0))\n\n        except Exception:\n            return 0.0\n\
          \n    def _evaluate_performance(train_score, oot_score, optimization_metric):\n\
          \        \"\"\"Performance evaluation\"\"\"\n        if optimization_metric.upper()\
          \ in ['AUC', 'AUCPR']:\n            excellent_threshold = 0.8\n        \
          \    good_threshold = 0.7\n            adequate_threshold = 0.6\n      \
          \  elif optimization_metric.upper() == 'F1':\n            excellent_threshold\
          \ = 0.75\n            good_threshold = 0.65\n            adequate_threshold\
          \ = 0.55\n        else:\n            excellent_threshold = 0.85\n      \
          \      good_threshold = 0.75\n            adequate_threshold = 0.65\n\n\
          \        if oot_score > excellent_threshold:\n            performance_status\
          \ = \"Model performs excellently\"\n        elif oot_score > good_threshold:\n\
          \            performance_status = \"Model performs well\"\n        elif\
          \ oot_score > adequate_threshold:\n            performance_status = \"Model\
          \ performs adequately\"\n        else:\n            performance_status =\
          \ \"Model needs improvement\"\n\n        overfit = train_score - oot_score\n\
          \        if overfit > 0.15:\n            overfit_status = \"severe overfitting\
          \ detected\"\n        elif overfit > 0.1:\n            overfit_status =\
          \ \"significant overfitting\"\n        elif overfit > 0.05:\n          \
          \  overfit_status = \"mild overfitting\"\n        else:\n            overfit_status\
          \ = \"acceptable generalization\"\n\n        return {\n            'status':\
          \ f\"{performance_status}, {overfit_status}\",\n            'train_score':\
          \ train_score,\n            'oot_score': oot_score,\n            'overfitting':\
          \ overfit,\n            'performance_level': performance_status,\n     \
          \       'overfitting_level': overfit_status\n        }\n\n    def _load_data_column_aware(file_path,\
          \ max_cols=None):\n        \"\"\"Column-aware data loading\"\"\"\n     \
          \   try:\n            # Metadata extraction\n            import pyarrow.parquet\
          \ as pq\n            parquet_file = pq.ParquetFile(file_path)\n        \
          \    total_rows = parquet_file.metadata.num_rows\n            schema = parquet_file.schema_arrow\n\
          \            total_cols = len(schema)\n\n            print(f\"Dataset: {total_rows}\
          \ rows \xD7 {total_cols} columns\")\n\n            # Wide dataset handling\n\
          \            if total_cols > 1000:\n                print(f\"Wide dataset\
          \ detected ({total_cols} columns) - using column-aware loading\")\n    \
          \            # Column chunking\n                if max_cols and total_cols\
          \ > max_cols:\n                    print(f\"Limiting to first {max_cols}\
          \ columns for memory efficiency\")\n                    # Schema column\
          \ extraction\n                    all_columns = [field.name for field in\
          \ schema]\n                    selected_cols = all_columns[:max_cols]\n\
          \                    df = pd.read_parquet(file_path, columns=selected_cols)\n\
          \                else:\n                    df = pd.read_parquet(file_path)\n\
          \            else:\n                df = pd.read_parquet(file_path)\n\n\
          \            print(f\"Loaded dataset: {df.shape}\")\n            return\
          \ df\n\n        except Exception as e:\n            print(f\"Error loading\
          \ {file_path}: {e}\")\n            # Basic loading fallback\n          \
          \  try:\n                # Sample for column info\n                df_sample\
          \ = pd.read_parquet(file_path).head(100)\n                safe_cols = df_sample.columns[:500].tolist()\
          \  # Limit to 500 columns\n                return pd.read_parquet(file_path,\
          \ columns=safe_cols)\n            except:\n                # Minimal data\
          \ fallback\n                df = pd.read_parquet(file_path)\n          \
          \      return df.head(10000)  # Take first 10k rows if all else fails\n\n\
          \    # Main execution\n    print(\"Loading input data with column-aware\
          \ approach...\")\n    train_df = _load_data_column_aware(train_input, max_cols=1000)\
          \  # Limit columns not rows\n    oot_df = _load_data_column_aware(oot_input,\
          \ max_cols=1000)     # Keep all rows\n    population_df = oot_df.copy()\
          \  # Using OOT data as population data\n\n    print(f\"Train data shape:\
          \ {train_df.shape}\")\n    print(f\"OOT data shape: {oot_df.shape}\")\n\
          \    print(f\"Population data shape: {population_df.shape}\")\n\n    # Column\
          \ validation\n    required_cols = ['partition_date', 'label']\n    for df_name,\
          \ df in [('population', population_df), ('train', train_df), ('oot', oot_df)]:\n\
          \        missing_cols = [col for col in required_cols if col not in df.columns]\n\
          \        if missing_cols:\n            raise ValueError(f\"Missing required\
          \ columns in {df_name} data: {missing_cols}\")\n\n    # H2O model loading\n\
          \    print(\"Initializing H2O...\")\n    h2o_available = _init_h2o()\n \
          \   model = None\n\n    if h2o_available:\n        try:\n            print(f\"\
          Loading H2O model from: {model_input}\")\n            model = h2o.load_model(model_input)\n\
          \            print(\"H2O model loaded successfully\")\n        except Exception\
          \ as e:\n            print(f\"Model loading failed: {str(e)}\")\n      \
          \      h2o_available = False\n            model = None\n\n    if not h2o_available\
          \ or model is None:\n        # Analysis without model\n        print(\"\
          H2O not available - providing comprehensive statistical analysis without\
          \ model predictions\")\n\n        # Basic monthly stats\n        try:\n\
          \            monthly_stats = {}\n            population_df_copy = population_df.copy()\n\
          \            population_df_copy['month'] = pd.to_datetime(population_df_copy['partition_date']).dt.to_period('M')\n\
          \n            for month in sorted(population_df_copy['month'].unique()):\n\
          \                month_data = population_df_copy[population_df_copy['month']\
          \ == month]\n                if len(month_data) > 0:\n                 \
          \   monthly_stats[str(month)] = {\n                        'samples': len(month_data),\n\
          \                        'positive_count': int(month_data['label'].sum()),\n\
          \                        'positive_rate': float(month_data['label'].mean()),\n\
          \                        'note': 'Basic statistics only - no model predictions\
          \ available'\n                    }\n        except Exception as e:\n  \
          \          monthly_stats = {'error': f'Monthly stats calculation failed:\
          \ {str(e)}'}\n\n        # Basic bin analysis\n        try:\n           \
          \ basic_bin_analysis = {}\n            population_df_copy = population_df.copy()\n\
          \            population_df_copy['month'] = pd.to_datetime(population_df_copy['partition_date']).dt.to_period('M')\n\
          \n            for month in sorted(population_df_copy['month'].unique()):\n\
          \                month_data = population_df_copy[population_df_copy['month']\
          \ == month]\n                if len(month_data) > 10:\n                \
          \    # Feature-based binning\n                    feature_cols = [c for\
          \ c in month_data.columns \n                                   if c not\
          \ in ['label', 'partition_date', 'risk_id', 'month']]\n                \
          \    if feature_cols:\n                        # Basic binning strategy\n\
          \                        main_feature = feature_cols[0]\n              \
          \          month_data['bin'] = pd.qcut(month_data[main_feature], q=5, labels=False,\
          \ duplicates='drop')\n\n                        bin_stats = []\n       \
          \                 total_population = len(month_data)\n                 \
          \       total_positive = int(month_data['label'].sum())\n\n            \
          \            for bin_num in sorted(month_data['bin'].unique()):\n      \
          \                      bin_data = month_data[month_data['bin'] == bin_num]\n\
          \                            bin_population = len(bin_data)\n          \
          \                  bin_positive = int(bin_data['label'].sum())\n\n     \
          \                       bin_stats.append({\n                           \
          \     'bin': int(bin_num),\n                                'count_population':\
          \ bin_population,\n                                'count_positive': bin_positive,\n\
          \                                'pct_distribution': round(bin_population\
          \ / total_population * 100, 2),\n                                'positive_rate':\
          \ round(bin_positive / bin_population * 100, 2) if bin_population > 0 else\
          \ 0\n                            })\n\n                        basic_bin_analysis[str(month)]\
          \ = {\n                            'total_population': total_population,\n\
          \                            'total_positive': total_positive,\n       \
          \                     'overall_positive_rate': round(total_positive / total_population\
          \ * 100, 2),\n                            'bins': bin_stats,\n         \
          \                   'note': f'Basic binning using feature: {main_feature}'\n\
          \                        }\n                    else:\n                \
          \        basic_bin_analysis[str(month)] = {'error': 'No features available\
          \ for binning'}\n                else:\n                    basic_bin_analysis[str(month)]\
          \ = {'error': 'Insufficient data for binning'}\n        except Exception\
          \ as e:\n            basic_bin_analysis = {'error': f'Basic bin analysis\
          \ failed: {str(e)}'}\n\n        # PSI analysis\n        try:\n         \
          \   psi_analysis = _calculate_psi_analysis(train_df, population_df)\n  \
          \      except Exception as e:\n            psi_analysis = {'error': f'PSI\
          \ analysis failed: {str(e)}'}\n\n        evaluation_summary = {\n      \
          \      'performance': _evaluate_performance(train_score, oot_score, optimization_metric),\n\
          \            'monthly_statistics': monthly_stats,\n            'basic_bin_analysis':\
          \ basic_bin_analysis,\n            'psi_analysis': psi_analysis,\n     \
          \       'warning': 'H2O model not available - analysis provided without\
          \ model predictions',\n            'evaluation_date': datetime.now().isoformat()\n\
          \        }\n\n        outputs = namedtuple('Outputs', ['evaluation_summary',\
          \ 'status'])\n        return outputs(json.dumps(evaluation_summary), \"\
          success\")\n\n    # Model-based analysis\n    try:\n        print(\"Starting\
          \ comprehensive model evaluation...\")\n\n        # Fallback date values\n\
          \        # Config parameter alternative\n        train_start_date = \"2024-01-01\"\
          \n        oot_end_date = \"2024-09-30\"\n\n        # Population data sampling\n\
          \        if len(population_df) > 30000:\n            print(f\"Further sampling\
          \ population data from {len(population_df)} to 30000 rows\")\n         \
          \   population_df = population_df.sample(n=30000, random_state=42)\n\n \
          \       # Monthly AUC analysis\n        print(\"Calculating monthly AUC...\"\
          )\n        monthly_auc = _calculate_monthly_auc(model, population_df, train_start_date,\
          \ oot_end_date)\n\n        # Early vs latest analysis  \n        print(\"\
          Calculating early vs latest AUC...\")\n        label_analysis = _calculate_early_vs_latest_auc(model,\
          \ population_df)\n\n        # Monthly bin analysis\n        print(\"Calculating\
          \ bin analysis...\")\n        bin_analysis = _calculate_bin_analysis(model,\
          \ population_df)\n\n        # PSI analysis\n        print(\"Calculating\
          \ PSI analysis...\")\n        psi_analysis = _calculate_psi_analysis(train_df.sample(n=min(10000,\
          \ len(train_df)), random_state=42), population_df)\n\n        # Performance\
          \ evaluation\n        performance_eval = _evaluate_performance(train_score,\
          \ oot_score, optimization_metric)\n\n        # Results compilation\n   \
          \     evaluation_summary = {\n            'performance': performance_eval,\n\
          \            'monthly_auc': monthly_auc,\n            'early_vs_latest_analysis':\
          \ label_analysis,\n            'bin_analysis': bin_analysis,\n         \
          \   'psi_analysis': psi_analysis,\n            'evaluation_date': datetime.now().isoformat()\n\
          \        }\n\n        print(\"Model evaluation completed successfully\"\
          )\n\n    except Exception as e:\n        print(f\"Error during model evaluation:\
          \ {str(e)}\")\n        evaluation_summary = {\n            'performance':\
          \ _evaluate_performance(train_score, oot_score, optimization_metric),\n\
          \            'error': f'Model evaluation failed: {str(e)}',\n          \
          \  'evaluation_date': datetime.now().isoformat()\n        }\n\n    finally:\n\
          \        # H2O cleanup\n        if h2o_available:\n            try:\n  \
          \              h2o.cluster().shutdown()\n                print(\"H2O cluster\
          \ shutdown completed\")\n            except Exception as e:\n          \
          \      print(f\"H2O shutdown warning: {str(e)}\")\n\n    outputs = namedtuple('Outputs',\
          \ ['evaluation_summary', 'status'])\n    return outputs(json.dumps(evaluation_summary),\
          \ \"success\")\n\n"
        image: nathangalung246/kubeflow_dummy:latest
        resources:
          cpuLimit: 4.0
          cpuRequest: 2.0
          memoryLimit: 8.589934592
          memoryRequest: 3.221225472
          resourceCpuLimit: '4'
          resourceCpuRequest: '2'
          resourceMemoryLimit: 8Gi
          resourceMemoryRequest: 3Gi
    exec-preprocess-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_data(\n    dataset_input: InputPath(),\n    train_output:\
          \ OutputPath(),\n    oot_output: OutputPath(),\n    feature_selection_report:\
          \ OutputPath(),\n    id_columns: list,\n    target_col: str,\n    date_col:\
          \ str,\n    ignored_features: list,\n    train_start: str,\n    train_end:\
          \ str,\n    oot_start: str,\n    oot_end: str\n) -> NamedTuple('Outputs',\
          \ [('train_shape', str), ('oot_shape', str), ('selected_features', str),\
          \ ('status', str)]):\n    import pandas as pd\n    import numpy as np\n\
          \    from collections import namedtuple\n    import json\n    import lightgbm\
          \ as lgb\n    import gc\n    import psutil\n    import pyarrow.parquet as\
          \ pq\n\n    def _optimize_dtypes(df):\n        \"\"\"Memory optimization\
          \ for DataFrame\"\"\"\n        print(f\"Memory usage before optimization:\
          \ {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n        for\
          \ col in df.columns:\n            col_type = df[col].dtype\n\n         \
          \   if col_type != 'object':\n                c_min = df[col].min()\n  \
          \              c_max = df[col].max()\n\n                if str(col_type)[:3]\
          \ == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max\
          \ < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n\
          \                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n\
          \                        df[col] = df[col].astype(np.int16)\n          \
          \          elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n\
          \                        df[col] = df[col].astype(np.int32)\n\n        \
          \        elif str(col_type)[:5] == 'float':\n                    if c_min\
          \ > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n   \
          \                     df[col] = df[col].astype(np.float32)\n\n        print(f\"\
          Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f}\
          \ MB\")\n        return df\n\n    def _monitor_memory():\n        \"\"\"\
          Memory usage monitoring\"\"\"\n        process = psutil.Process()\n    \
          \    memory_info = process.memory_info()\n        memory_mb = memory_info.rss\
          \ / 1024 / 1024\n\n        # Memory thresholds - adjusted for 5GB pod limit\n\
          \        warning_threshold = 3000   # 3GB warning  \n        critical_threshold\
          \ = 4500  # 4.5GB critical (within 5GB pod limit)\n\n        if memory_mb\
          \ > critical_threshold:\n            print(f\"CRITICAL: Memory usage {memory_mb:.2f}\
          \ MB exceeds {critical_threshold} MB - terminating to prevent system crash\"\
          )\n            raise MemoryError(f\"Memory usage {memory_mb:.2f} MB exceeds\
          \ critical threshold {critical_threshold} MB\")\n        elif memory_mb\
          \ > warning_threshold:\n            print(f\"WARNING: High memory usage\
          \ {memory_mb:.2f} MB (threshold: {warning_threshold} MB)\")\n        else:\n\
          \            print(f\"Current memory usage: {memory_mb:.2f} MB\")\n\n  \
          \      return memory_mb\n\n    def _validate_data(df, id_columns, target_col,\
          \ date_col, train_start, train_end, oot_start, oot_end):\n        \"\"\"\
          Dataset validation checks\"\"\"\n\n        # Check duplicates using ID +\
          \ target\n        check_cols = id_columns + [target_col]\n        duplicates\
          \ = df[check_cols].duplicated().sum()\n        if duplicates > 0:\n    \
          \        raise ValueError(f\"Found {duplicates} duplicate rows\")\n\n  \
          \      # Check target has only 2 values\n        unique_targets = df[target_col].nunique()\n\
          \        if unique_targets != 2:\n            raise ValueError(f\"Target\
          \ must have 2 values, found {unique_targets}\")\n\n        # Check date\
          \ range\n        df[date_col] = pd.to_datetime(df[date_col])\n        min_date\
          \ = df[date_col].min()\n        max_date = df[date_col].max()\n\n      \
          \  # Check if we have data for training period\n        if min_date > pd.to_datetime(train_start)\
          \ or max_date < pd.to_datetime(train_end):\n            raise ValueError(f\"\
          Data range {min_date} to {max_date} doesn't cover training period {train_start}\
          \ to {train_end}\")\n\n        # Check if we have some OOT data (warn if\
          \ partial)\n        oot_start_dt = pd.to_datetime(oot_start)\n        oot_end_dt\
          \ = pd.to_datetime(oot_end)\n\n        if max_date < oot_start_dt:\n   \
          \         raise ValueError(f\"No OOT data available. Data ends at {max_date}\
          \ but OOT starts at {oot_start}\")\n        elif max_date < oot_end_dt:\n\
          \            print(f\"WARNING: OOT period is partially covered. Data ends\
          \ at {max_date} but OOT should end at {oot_end}\")\n\n        # Check column\
          \ names are alphanumeric + underscore\n        for col in df.columns:\n\
          \            if not col.replace('_', '').isalnum():\n                raise\
          \ ValueError(f\"Column '{col}' contains invalid characters\")\n\n      \
          \  print(\"Data validation passed\")\n\n    def _basic_preprocessing(df,\
          \ id_columns, target_col, date_col, ignored_features):\n        \"\"\"Data\
          \ preprocessing operations\"\"\"\n\n        # Select relevant columns\n\
          \        use_cols = [c for c in df.columns \n                    if c in\
          \ id_columns + [target_col, date_col] \n                    or (c not in\
          \ ignored_features and c not in id_columns and c != target_col and c !=\
          \ date_col)]\n        df = df[use_cols]\n\n        # Remove zero standard\
          \ deviation columns\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\
          \        numeric_cols = [c for c in numeric_cols if c not in id_columns\
          \ + [target_col]]\n\n        if len(numeric_cols) > 0:\n            stds\
          \ = df[numeric_cols].std()\n            zero_std_cols = stds[stds == 0].index.tolist()\n\
          \            if zero_std_cols:\n                df = df.drop(columns=zero_std_cols)\n\
          \                print(f\"Removed {len(zero_std_cols)} zero std columns\"\
          )\n\n        # Fill missing values\n        df = df.fillna(-999999)\n\n\
          \        # Skip rounding to save memory for very wide datasets\n       \
          \ if len(df.columns) < 1000:\n            # Round to 2 decimal places only\
          \ for smaller datasets\n            numeric_cols = df.select_dtypes(include=[np.number]).columns\n\
          \            df[numeric_cols] = df[numeric_cols].round(2)\n        else:\n\
          \            print(f\"Skipping rounding for wide dataset ({len(df.columns)}\
          \ columns) to save memory\")\n\n        # Ensure date column is datetime\n\
          \        df[date_col] = pd.to_datetime(df[date_col])\n\n        return df\n\
          \n    def _train_test_split(df, date_col, train_start, train_end, oot_start,\
          \ oot_end):\n        \"\"\"Train OOT temporal split\"\"\"\n\n        train_mask\
          \ = (df[date_col] >= train_start) & (df[date_col] <= train_end)\n      \
          \  oot_mask = (df[date_col] >= oot_start) & (df[date_col] <= oot_end)\n\n\
          \        train_df = df[train_mask].copy()\n        oot_df = df[oot_mask].copy()\n\
          \n        print(f\"Train period: {train_start} to {train_end} - {len(train_df):,}\
          \ samples\")\n        print(f\"OOT period: {oot_start} to {oot_end} - {len(oot_df):,}\
          \ samples\")\n\n        return train_df, oot_df\n\n    def _statistical_feature_analysis(train_df,\
          \ oot_df, target_col, id_columns, date_col):\n        \"\"\"Statistical\
          \ feature selection\"\"\"\n        import scipy.stats as stats\n       \
          \ from sklearn.feature_selection import mutual_info_classif, chi2, SelectKBest,\
          \ f_classif\n        from sklearn.preprocessing import LabelEncoder, StandardScaler\n\
          \        from sklearn.ensemble import RandomForestClassifier\n        from\
          \ sklearn.metrics import roc_auc_score\n        import warnings\n      \
          \  warnings.filterwarnings('ignore')\n\n        feature_cols = [c for c\
          \ in train_df.columns \n                       if c not in id_columns +\
          \ [target_col, date_col]]\n\n        if len(feature_cols) == 0:\n      \
          \      return []\n\n        print(f\"Starting enhanced statistical analysis\
          \ of {len(feature_cols)} features for improved AUCPR...\")\n\n        #\
          \ Step 0: Statistical Data Quality Pre-filtering\n        print(\"Step 0:\
          \ Statistical data quality pre-filtering...\")\n\n        def assess_feature_quality(feature_data,\
          \ target_data):\n            \"\"\"Feature quality assessment\"\"\"\n  \
          \          try:\n                # 1. Missing value analysis - more lenient\
          \ for better features\n                missing_ratio = feature_data.isnull().sum()\
          \ / len(feature_data)\n                if missing_ratio > 0.90:  # Increased\
          \ from 0.95\n                    return False, f\"Excessive missing values:\
          \ {missing_ratio:.2%}\"\n\n                # 2. For numeric features\n \
          \               if pd.api.types.is_numeric_dtype(feature_data):\n      \
          \              feature_clean = feature_data.dropna()\n                 \
          \   target_clean = target_data[~feature_data.isnull()]\n\n             \
          \       if len(feature_clean) < 10:\n                        return False,\
          \ \"Insufficient valid values\"\n\n                    # Check for zero\
          \ variance\n                    variance = feature_clean.var()\n       \
          \             if variance == 0:\n                        return False, \"\
          Zero variance (constant values)\"\n\n                    # Enhanced variability\
          \ check with target correlation\n                    try:\n            \
          \            # Quick correlation check - keep features with any correlation\n\
          \                        corr = np.corrcoef(feature_clean.fillna(0), target_clean)[0,\
          \ 1]\n                        if not np.isnan(corr) and abs(corr) > 0.01:\
          \  # Very low threshold\n                            return True, \"Target\
          \ correlated\"\n                    except:\n                        pass\n\
          \n                    # Coefficient of variation - more lenient\n      \
          \              mean_val = feature_clean.mean()\n                    if mean_val\
          \ != 0:\n                        cv = abs(variance**0.5 / mean_val)\n  \
          \                      if cv < 0.001:  # Reduced from 0.005\n          \
          \                  return False, f\"Very low variability (CV: {cv:.6f})\"\
          \n\n                # 3. For categorical features - enhanced assessment\n\
          \                elif pd.api.types.is_object_dtype(feature_data):\n    \
          \                feature_clean = feature_data.dropna()\n               \
          \     target_clean = target_data[~feature_data.isnull()]\n\n           \
          \         if len(feature_clean) < 10:\n                        return False,\
          \ \"Insufficient valid values\"\n\n                    unique_count = feature_clean.nunique()\n\
          \                    if unique_count <= 1:\n                        return\
          \ False, \"Single unique value\"\n\n                    # More lenient cardinality\
          \ check\n                    cardinality_ratio = unique_count / len(feature_clean)\n\
          \                    if cardinality_ratio > 0.98:  # Increased from 0.95\n\
          \                        return False, f\"High cardinality: {unique_count}\
          \ unique values\"\n\n                    # Quick target interaction check\
          \ for categorical\n                    try:\n                        if\
          \ 2 <= unique_count <= 20:  # Reasonable categories\n                  \
          \          contingency = pd.crosstab(feature_clean, target_clean)\n    \
          \                        if contingency.shape[0] > 1 and contingency.shape[1]\
          \ > 1:\n                                chi2_stat, p_value = stats.chi2_contingency(contingency)[:2]\n\
          \                                if p_value < 0.2:  # Lenient significance\n\
          \                                    return True, \"Target associated\"\n\
          \                    except:\n                        pass\n\n         \
          \       return True, \"Pass\"\n\n            except Exception as e:\n  \
          \              return False, f\"Error in assessment: {str(e)}\"\n\n    \
          \    # Enhanced pre-filtering with target interaction\n        quality_features\
          \ = []\n        removed_count = 0\n\n        # Combined target for assessment\n\
          \        combined_target = pd.concat([train_df[target_col], oot_df[target_col]],\
          \ ignore_index=True)\n\n        # Process in small batches to avoid memory\
          \ spikes\n        batch_size = 50  # Increased batch size for efficiency\n\
          \        for i in range(0, len(feature_cols), batch_size):\n           \
          \ batch_features = feature_cols[i:i + batch_size]\n\n            for feature\
          \ in batch_features:\n                try:\n                    # Use combined\
          \ data for robust assessment\n                    combined_feature_data\
          \ = pd.concat([train_df[feature], oot_df[feature]], ignore_index=True)\n\
          \                    is_quality, reason = assess_feature_quality(combined_feature_data,\
          \ combined_target)\n\n                    if is_quality:\n             \
          \           quality_features.append(feature)\n                    else:\n\
          \                        removed_count += 1\n                        if\
          \ removed_count <= 5:  # Show fewer removals\n                         \
          \   print(f\"  Removed {feature}: {reason}\")\n                        elif\
          \ removed_count == 6:\n                            print(f\"  ... (continuing\
          \ quality filtering)\")\n\n                except Exception as e:\n    \
          \                removed_count += 1\n\n            # Memory cleanup after\
          \ each batch\n            if i % 200 == 0:  # Less frequent cleanup\n  \
          \              gc.collect()\n\n        print(f\"Statistical pre-filtering:\
          \ {len(feature_cols)} -> {len(quality_features)} features\")\n\n       \
          \ # Update feature list to quality-filtered features\n        feature_cols\
          \ = quality_features\n\n        if len(feature_cols) == 0:\n           \
          \ print(\"No features passed quality pre-filtering\")\n            return\
          \ []\n\n        # Enhanced statistical significance testing\n        if\
          \ len(feature_cols) > 300:\n            print(f\"Still {len(feature_cols)}\
          \ features - applying enhanced statistical filter...\")\n\n            #\
          \ Multiple statistical tests for better feature selection\n            significant_features\
          \ = []\n            y_combined = pd.concat([train_df[target_col], oot_df[target_col]],\
          \ ignore_index=True)\n\n            # Process more features with multiple\
          \ tests\n            for feature in feature_cols[:800]:  # Increased from\
          \ 500\n                try:\n                    combined_feature = pd.concat([train_df[feature],\
          \ oot_df[feature]], ignore_index=True)\n                    feature_score\
          \ = 0\n\n                    if pd.api.types.is_numeric_dtype(combined_feature):\n\
          \                        # Multiple correlation tests\n                \
          \        feature_clean = combined_feature.fillna(combined_feature.median())\n\
          \n                        # Pearson correlation\n                      \
          \  try:\n                            corr = np.corrcoef(feature_clean, y_combined)[0,\
          \ 1]\n                            if not np.isnan(corr):\n             \
          \                   feature_score += abs(corr) * 10\n                  \
          \      except:\n                            pass\n\n                   \
          \     # Spearman correlation for non-linear relationships\n            \
          \            try:\n                            spearman_corr, _ = stats.spearmanr(feature_clean,\
          \ y_combined)\n                            if not np.isnan(spearman_corr):\n\
          \                                feature_score += abs(spearman_corr) * 8\n\
          \                        except:\n                            pass\n\n \
          \                       # F-statistic test\n                        try:\n\
          \                            f_stat, p_val = f_classif(feature_clean.values.reshape(-1,\
          \ 1), y_combined)\n                            if p_val[0] < 0.05:\n   \
          \                             feature_score += 5\n                     \
          \   except:\n                            pass\n\n                    elif\
          \ pd.api.types.is_object_dtype(combined_feature):\n                    \
          \    # Enhanced categorical tests\n                        try:\n      \
          \                      contingency = pd.crosstab(combined_feature.fillna('missing'),\
          \ y_combined)\n                            if contingency.shape[0] > 1 and\
          \ contingency.shape[1] > 1:\n                                chi2, p_value,\
          \ _, _ = stats.chi2_contingency(contingency)\n                         \
          \       if p_value < 0.05:  # More stringent\n                         \
          \           feature_score += 8\n                                elif p_value\
          \ < 0.1:\n                                    feature_score += 4\n     \
          \                   except:\n                            pass\n\n      \
          \                  # Mutual information for categorical\n              \
          \          try:\n                            le = LabelEncoder()\n     \
          \                       feature_encoded = le.fit_transform(combined_feature.fillna('missing').astype(str))\n\
          \                            mi_score = mutual_info_classif(feature_encoded.reshape(-1,\
          \ 1), y_combined, random_state=42)[0]\n                            feature_score\
          \ += mi_score * 20\n                        except:\n                  \
          \          pass\n\n                    # Keep features with any significant\
          \ score\n                    if feature_score > 0.05:  # Lower threshold\
          \ to keep more features\n                        significant_features.append((feature,\
          \ feature_score))\n\n                except:\n                    continue\n\
          \n                # Dynamic stopping based on quality\n                if\
          \ len(significant_features) >= 400:  # Increased limit\n               \
          \     break\n\n            # Sort by score and take top features\n     \
          \       significant_features.sort(key=lambda x: x[1], reverse=True)\n  \
          \          feature_cols = [f[0] for f in significant_features[:300]]  #\
          \ Top 300\n            print(f\"After enhanced statistical testing: {len(feature_cols)}\
          \ features\")\n\n        feature_scores = {}\n\n        # Combine train\
          \ and oot for stability analysis\n        combined_df = pd.concat([train_df,\
          \ oot_df], ignore_index=True)\n\n        print(\"1. Analyzing feature stability\
          \ between train and OOT...\")\n\n        # 1. Feature Stability Analysis\
          \ (PSI-based)\n        def calculate_psi(expected, actual, bins=10):\n \
          \           \"\"\"Population Stability Index calculation\"\"\"\n       \
          \     try:\n                expected_clean = expected.dropna()\n       \
          \         actual_clean = actual.dropna()\n\n                if len(expected_clean)\
          \ < 10 or len(actual_clean) < 10:\n                    return float('inf')\
          \  # Unstable\n\n                # Create bins based on expected (train)\
          \ distribution\n                try:\n                    bin_edges = np.quantile(expected_clean,\
          \ np.linspace(0, 1, bins + 1))\n                    bin_edges = np.unique(bin_edges)\n\
          \                    if len(bin_edges) < 3:\n                        return\
          \ float('inf')\n                except:\n                    return float('inf')\n\
          \n                # Calculate histograms\n                expected_hist,\
          \ _ = np.histogram(expected_clean, bins=bin_edges)\n                actual_hist,\
          \ _ = np.histogram(actual_clean, bins=bin_edges)\n\n                # Avoid\
          \ division by zero\n                expected_pct = (expected_hist + 1e-8)\
          \ / (expected_hist.sum() + 1e-7)\n                actual_pct = (actual_hist\
          \ + 1e-8) / (actual_hist.sum() + 1e-7)\n\n                # PSI calculation\n\
          \                psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct\
          \ / expected_pct))\n                return max(0.0, min(psi, 10.0))  # Cap\
          \ at 10\n            except:\n                return float('inf')\n\n  \
          \      stable_features = []\n\n        # Process features in batches to\
          \ manage memory\n        batch_size = 50\n        for i in range(0, len(feature_cols),\
          \ batch_size):\n            batch_features = feature_cols[i:i+batch_size]\n\
          \n            for feature in batch_features:\n                try:\n   \
          \                 train_values = train_df[feature]\n                   \
          \ oot_values = oot_df[feature]\n\n                    # Check if feature\
          \ has sufficient variance\n                    combined_values = combined_df[feature]\n\
          \n                    # Skip if too many nulls\n                    null_ratio\
          \ = combined_values.isnull().sum() / len(combined_values)\n            \
          \        if null_ratio > 0.9:\n                        continue\n\n    \
          \                # For numeric features\n                    if combined_values.dtype\
          \ in ['float64', 'float32', 'int64', 'int32']:\n                       \
          \ # Check variance\n                        if combined_values.var() ==\
          \ 0:\n                            continue\n\n                        #\
          \ Calculate PSI with more lenient thresholds for better feature retention\n\
          \                        psi = calculate_psi(train_values, oot_values)\n\
          \                        if psi < 0.4:  # More lenient PSI threshold (was\
          \ 0.25)\n                            stable_features.append((feature, psi,\
          \ 'numeric'))\n                        elif psi < 1.0:  # Include moderately\
          \ unstable features if they have predictive power\n                    \
          \        # Check if feature has strong correlation despite instability\n\
          \                            try:\n                                combined_feature\
          \ = pd.concat([train_values, oot_values], ignore_index=True)\n         \
          \                       combined_target = pd.concat([train_df[target_col],\
          \ oot_df[target_col]], ignore_index=True)\n                            \
          \    corr = np.corrcoef(combined_feature.fillna(0), combined_target)[0,\
          \ 1]\n                                if not np.isnan(corr) and abs(corr)\
          \ > 0.05:  # Strong predictive power\n                                 \
          \   stable_features.append((feature, psi, 'numeric'))\n                \
          \            except:\n                                pass\n\n         \
          \           # For categorical features\n                    elif combined_values.dtype\
          \ == 'object':\n                        # Check if categorical has reasonable\
          \ number of categories\n                        n_categories = combined_values.nunique()\n\
          \                        if 2 <= n_categories <= 50:  # Reasonable range\n\
          \                            # Compare distributions using Chi-square test\n\
          \                            try:\n                                train_counts\
          \ = train_values.value_counts()\n                                oot_counts\
          \ = oot_values.value_counts()\n\n                                # Align\
          \ categories\n                                all_categories = set(train_counts.index)\
          \ | set(oot_counts.index)\n                                train_aligned\
          \ = [train_counts.get(cat, 0) for cat in all_categories]\n             \
          \                   oot_aligned = [oot_counts.get(cat, 0) for cat in all_categories]\n\
          \n                                # Chi-square test for independence - more\
          \ lenient thresholds\n                                if sum(train_aligned)\
          \ > 0 and sum(oot_aligned) > 0:\n                                    chi2_stat,\
          \ p_value = stats.chisquare(oot_aligned, train_aligned)\n              \
          \                      if p_value > 0.01:  # More lenient threshold (was\
          \ 0.05)\n                                        stable_features.append((feature,\
          \ p_value, 'categorical'))\n                                    else:\n\
          \                                        # Even if distributions differ,\
          \ keep if predictive\n                                        try:\n   \
          \                                         le = LabelEncoder()\n        \
          \                                    combined_feature = pd.concat([train_values,\
          \ oot_values], ignore_index=True)\n                                    \
          \        combined_target = pd.concat([train_df[target_col], oot_df[target_col]],\
          \ ignore_index=True)\n                                            feature_encoded\
          \ = le.fit_transform(combined_feature.fillna('missing').astype(str))\n \
          \                                           mi_score = mutual_info_classif(feature_encoded.reshape(-1,\
          \ 1), combined_target, random_state=42)[0]\n                           \
          \                 if mi_score > 0.05:  # Strong mutual information\n   \
          \                                             stable_features.append((feature,\
          \ p_value, 'categorical'))\n                                        except:\n\
          \                                            pass\n                    \
          \        except:\n                                continue\n\n         \
          \       except:\n                    continue\n\n            # Memory cleanup\
          \ after each batch\n            gc.collect()\n\n        print(f\"Found {len(stable_features)}\
          \ stable features\")\n\n        # 2. Enhanced Target Relationship Analysis\n\
          \        print(\"2. Analyzing enhanced target relationships...\")\n\n  \
          \      target_correlated_features = []\n        y_train = train_df[target_col]\n\
          \        y_oot = oot_df[target_col]\n\n        # Process features in batches\
          \ for memory efficiency\n        batch_size = 100\n        for i in range(0,\
          \ len(stable_features), batch_size):\n            batch_features = stable_features[i:i+batch_size]\n\
          \n            for feature, stability_score, feature_type in batch_features:\n\
          \                try:\n                    feature_scores = []\n\n     \
          \               if feature_type == 'numeric':\n                        train_feature\
          \ = train_df[feature].fillna(train_df[feature].median())\n             \
          \           oot_feature = oot_df[feature].fillna(oot_df[feature].median())\n\
          \n                        # Multiple correlation measures for robustness\n\
          \                        # Pearson correlation\n                       \
          \ try:\n                            corr_train = stats.pearsonr(train_feature,\
          \ y_train)[0]\n                            corr_oot = stats.pearsonr(oot_feature,\
          \ y_oot)[0]\n                            if not np.isnan(corr_train) and\
          \ not np.isnan(corr_oot):\n                                avg_corr = (abs(corr_train)\
          \ + abs(corr_oot)) / 2\n                                feature_scores.append(avg_corr)\n\
          \                        except:\n                            pass\n\n \
          \                       # Spearman correlation for non-linear relationships\n\
          \                        try:\n                            spearman_train,\
          \ _ = stats.spearmanr(train_feature, y_train)\n                        \
          \    spearman_oot, _ = stats.spearmanr(oot_feature, y_oot)\n           \
          \                 if not np.isnan(spearman_train) and not np.isnan(spearman_oot):\n\
          \                                avg_spearman = (abs(spearman_train) + abs(spearman_oot))\
          \ / 2\n                                feature_scores.append(avg_spearman\
          \ * 0.8)  # Weight slightly less\n                        except:\n    \
          \                        pass\n\n                        # Point-biserial\
          \ correlation (specific for binary targets)\n                        try:\n\
          \                            pb_corr, _ = stats.pointbiserialr(y_train,\
          \ train_feature)\n                            if not np.isnan(pb_corr):\n\
          \                                feature_scores.append(abs(pb_corr))\n \
          \                       except:\n                            pass\n\n  \
          \                  elif feature_type == 'categorical':\n               \
          \         try:\n                            # Enhanced categorical analysis\n\
          \                            le = LabelEncoder()\n                     \
          \       train_encoded = le.fit_transform(train_df[feature].fillna('missing').astype(str))\n\
          \n                            # Mutual information\n                   \
          \         mi_score = mutual_info_classif(train_encoded.reshape(-1, 1), y_train,\
          \ random_state=42)[0]\n                            feature_scores.append(mi_score\
          \ * 2)  # Weight higher for MI\n\n                            # Chi-square\
          \ test\n                            contingency = pd.crosstab(train_df[feature].fillna('missing'),\
          \ y_train)\n                            if contingency.shape[0] > 1 and\
          \ contingency.shape[1] > 1:\n                                chi2_stat,\
          \ p_value, _, _ = stats.chi2_contingency(contingency)\n                \
          \                if p_value < 0.05:\n                                  \
          \  feature_scores.append(0.1)  # Bonus for significance\n              \
          \          except:\n                            continue\n\n           \
          \         # Take maximum score from all measures\n                    if\
          \ feature_scores:\n                        max_score = max(feature_scores)\n\
          \                        if max_score > 0.01:  # Reduced threshold\n   \
          \                         target_correlated_features.append((feature, max_score,\
          \ stability_score, feature_type))\n\n                except:\n         \
          \           continue\n\n            # Memory cleanup\n            if i %\
          \ 300 == 0:\n                gc.collect()\n\n        print(f\"Found {len(target_correlated_features)}\
          \ target-correlated features\")\n\n        # 3. Advanced Final Selection\
          \ with ML-Based Scoring\n        print(\"3. Computing advanced feature scores\
          \ with ML validation...\")\n\n        # Enhanced combined scoring\n    \
          \    final_scores = []\n        for feature, target_score, stability_score,\
          \ feature_type in target_correlated_features:\n            if feature_type\
          \ == 'numeric':\n                # For numeric: higher target score, lower\
          \ PSI is better\n                combined_score = target_score * (2 / (1\
          \ + stability_score))\n            else:\n                # For categorical:\
          \ higher target score, higher stability p-value is better\n            \
          \    combined_score = target_score * min(stability_score * 2, 1.0)\n\n \
          \           final_scores.append((feature, combined_score, feature_type,\
          \ target_score))\n\n        # Sort by combined score\n        final_scores.sort(key=lambda\
          \ x: x[1], reverse=True)\n\n        # ML-based validation for top features\n\
          \        if len(final_scores) > 50:\n            print(f\"Validating top\
          \ features with Random Forest...\")\n\n            # Take top candidates\
          \ for ML validation\n            top_candidates = [f[0] for f in final_scores[:min(1000,\
          \ len(final_scores))]]\n\n            try:\n                # Prepare data\
          \ for ML validation\n                X_train_sample = train_df[top_candidates].fillna(-999999).head(min(10000,\
          \ len(train_df)))\n                y_train_sample = train_df[target_col].head(min(10000,\
          \ len(train_df)))\n\n                # Quick Random Forest for feature importance\n\
          \                rf = RandomForestClassifier(n_estimators=20, max_depth=6,\
          \ random_state=42, n_jobs=1)\n                rf.fit(X_train_sample, y_train_sample)\n\
          \n                # Get feature importance and combine with statistical\
          \ scores\n                rf_importance = dict(zip(top_candidates, rf.feature_importances_))\n\
          \n                # Re-score with ML validation\n                ml_enhanced_scores\
          \ = []\n                for feature, stat_score, ftype, target_score in\
          \ final_scores:\n                    if feature in rf_importance:\n    \
          \                    # Combine statistical score with ML importance\n  \
          \                      ml_score = rf_importance[feature]\n             \
          \           enhanced_score = (stat_score * 0.6) + (ml_score * 0.4)\n   \
          \                     ml_enhanced_scores.append((feature, enhanced_score,\
          \ ftype))\n\n                final_scores = [(f, score, ftype, 0) for f,\
          \ score, ftype in ml_enhanced_scores]\n                final_scores.sort(key=lambda\
          \ x: x[1], reverse=True)\n\n                print(f\"ML validation completed\
          \ for {len(ml_enhanced_scores)} features\")\n\n            except Exception\
          \ as e:\n                print(f\"ML validation failed: {e}, using statistical\
          \ scores only\")\n\n        # Optimized selection focused on stability and\
          \ predictive power\n        if len(final_scores) > 0:\n            # Strategy:\
          \ Keep all features that show ANY predictive power and are stable\n    \
          \        # Remove arbitrary limits that may hurt model performance\n\n \
          \           scores_only = [score for _, score, _, _ in final_scores]\n \
          \           score_mean = np.mean(scores_only)\n            score_std = np.std(scores_only)\n\
          \n            # Use a very permissive threshold - keep features with any\
          \ statistical significance\n            # Focus on removing only clearly\
          \ useless features\n            min_meaningful_score = max(0.001, score_mean\
          \ - 2 * score_std)  # Very permissive\n\n            # Separate by stability\
          \ (PSI) for different treatment\n            high_stability_features = []\
          \  # PSI < 0.25 (very stable)\n            moderate_stability_features =\
          \ []  # PSI 0.25-0.5 (moderately stable)\n\n            for feature, score,\
          \ ftype, target_score in final_scores:\n                if score > min_meaningful_score:\
          \  # Any meaningful predictive power\n                    # Find the stability\
          \ score from our earlier analysis\n                    stability_info =\
          \ None\n                    for stable_feat, stability_score, feat_type\
          \ in stable_features:\n                        if stable_feat == feature:\n\
          \                            stability_info = (stability_score, feat_type)\n\
          \                            break\n\n                    if stability_info:\n\
          \                        stability_score, feat_type = stability_info\n \
          \                       if feat_type == 'numeric' and stability_score <\
          \ 0.4:  # Stable (updated threshold)\n                            high_stability_features.append(feature)\n\
          \                        elif feat_type == 'numeric' and stability_score\
          \ < 1.0:  # Moderately stable\n                            moderate_stability_features.append(feature)\n\
          \                        elif feat_type == 'categorical' and stability_score\
          \ > 0.01:  # Categorical with reasonable stability\n                   \
          \         high_stability_features.append(feature)\n                    \
          \    elif feat_type == 'categorical':  # Include all categorical if they\
          \ have predictive power\n                            moderate_stability_features.append(feature)\n\
          \                    else:\n                        # If no stability info,\
          \ include if it has strong predictive power\n                        if\
          \ score > score_mean:\n                            moderate_stability_features.append(feature)\n\
          \n            # Build final selection prioritizing stable features\n   \
          \         selected_features = high_stability_features.copy()\n\n       \
          \     # Add moderate features if we have room and they add value\n     \
          \       for feat in moderate_stability_features:\n                if feat\
          \ not in selected_features:\n                    selected_features.append(feat)\n\
          \n            # If still too few features, add more from final_scores\n\
          \            if len(selected_features) < 20:  # Very minimal threshold\n\
          \                for feature, score, ftype, target_score in final_scores:\n\
          \                    if feature not in selected_features and score > 0:\n\
          \                        selected_features.append(feature)\n           \
          \             if len(selected_features) >= 50:  # Reasonable minimum\n \
          \                           break\n\n            # Only apply upper limit\
          \ if memory is truly constraining\n            # Allow up to 500 features\
          \ if they're all meaningful\n            if len(selected_features) > 500:\n\
          \                # Sort by combined score and take top 500\n           \
          \     sorted_features = sorted([(f, s) for f, s, _, _ in final_scores if\
          \ f in selected_features], \n                                       key=lambda\
          \ x: x[1], reverse=True)\n                selected_features = [f[0] for\
          \ f in sorted_features[:500]]\n                print(f\"Reduced to top 500\
          \ features due to memory constraints\")\n\n            print(f\"Selected\
          \ {len(selected_features)} features using stability-focused selection\"\
          )\n            print(f\"  - High stability features: {len(high_stability_features)}\"\
          )\n            print(f\"  - Moderate stability features: {len(moderate_stability_features)}\"\
          )\n        else:\n            selected_features = []\n\n        print(f\"\
          Final selection: {len(selected_features)} features using enhanced statistical\
          \ analysis\")\n\n        # Print top features with their scores\n      \
          \  print(\"Top 15 selected features:\")\n        for i, (feature, score,\
          \ ftype, target_score) in enumerate(final_scores[:15]):\n            status\
          \ = \"SELECTED\" if feature in selected_features else \"REJECTED\"\n   \
          \         print(f\"  {status} {i+1}. {feature} ({ftype}): combined={score:.6f},\
          \ target={target_score:.6f}\")\n\n        return selected_features\n\n \
          \   def _feature_selection_lgbm_batched(train_df, target_col, id_columns,\
          \ date_col, max_features=1000):\n        \"\"\"LightGBM feature selection\
          \ fallback\"\"\"\n\n        feature_cols = [c for c in train_df.columns\
          \ \n                       if c not in id_columns + [target_col, date_col]]\n\
          \n        if len(feature_cols) == 0:\n            return []\n\n        #\
          \ Conservative feature limit based on memory constraints\n        max_features_for_memory\
          \ = min(max_features, 1000)  # Conservative limit for memory efficiency\n\
          \n        if len(feature_cols) > max_features_for_memory:\n            print(f\"\
          MEMORY CRITICAL: {len(feature_cols)} features detected, reducing to {max_features_for_memory}\
          \ to prevent OOM\")\n\n            # Ultra-fast feature selection - only\
          \ check first 500 features to avoid memory issues\n            print(f\"\
          Quick filtering first 500 features to avoid memory spike...\")\n       \
          \     valid_features = []\n\n            # Process in smaller batches to\
          \ avoid memory spikes\n            batch_size = 100\n            for i in\
          \ range(0, min(500, len(feature_cols)), batch_size):\n                batch_cols\
          \ = feature_cols[i:i+batch_size]\n                for col in batch_cols:\n\
          \                    try:\n                        # Quick checks without\
          \ loading too much data\n                        if str(train_df[col].dtype)\
          \ in ['float64', 'float32', 'int64', 'int32']:\n                       \
          \     # Sample check to avoid full column scan\n                       \
          \     sample_data = train_df[col].head(1000)\n                         \
          \   null_ratio = sample_data.isnull().sum() / len(sample_data)\n       \
          \                     if null_ratio < 0.8:  # Less than 80% nulls in sample\n\
          \                                var_val = sample_data.var()\n         \
          \                       if pd.notna(var_val) and var_val > 0:\n        \
          \                            valid_features.append(col)\n              \
          \                      if len(valid_features) >= max_features_for_memory:\n\
          \                                        break\n                    except:\n\
          \                        continue\n\n                if len(valid_features)\
          \ >= max_features_for_memory:\n                    break\n\n           \
          \     # Force garbage collection after each batch\n                gc.collect()\n\
          \n            print(f\"Selected {len(valid_features)} features using ultra-fast\
          \ selection\")\n            feature_cols = valid_features[:max_features_for_memory]\n\
          \n        print(f\"Starting batched feature selection with {len(feature_cols)}\
          \ features on {len(train_df)} samples...\")\n\n        # Sample data if\
          \ too large to fit in memory\n        sample_size = min(50000, len(train_df))\n\
          \        if len(train_df) > sample_size:\n            print(f\"Sampling\
          \ {sample_size} rows for feature selection\")\n            train_sample\
          \ = train_df.sample(n=sample_size, random_state=42)\n        else:\n   \
          \         train_sample = train_df\n\n        X = train_sample[feature_cols].fillna(-999999)\n\
          \        y = train_sample[target_col]\n\n        # Use faster LightGBM settings\
          \ for large datasets\n        lgb_train = lgb.Dataset(X, label=y)\n    \
          \    params = {\n            'objective': 'binary',\n            'metric':\
          \ 'auc',\n            'boosting_type': 'gbdt',\n            'num_leaves':\
          \ 10,  # Further reduced for speed\n            'learning_rate': 0.2,  #\
          \ Increased for faster convergence\n            'feature_fraction': 0.6,\
          \  # Reduced to handle more features\n            'bagging_fraction': 0.8,\n\
          \            'bagging_freq': 5,\n            'verbose': -1,\n          \
          \  'force_col_wise': True,\n            'max_depth': 4  # Limit depth for\
          \ speed\n        }\n\n        print(\"Training LightGBM for feature importance\
          \ (fast mode)...\")\n        model = lgb.train(params, lgb_train, num_boost_round=3,\
          \ callbacks=[lgb.log_evaluation(0)])\n\n        # Get feature importance\n\
          \        importance = model.feature_importance()\n        feature_imp =\
          \ list(zip(feature_cols, importance))\n        feature_imp.sort(key=lambda\
          \ x: x[1], reverse=True)\n\n        print(f\"Feature importance calculated.\
          \ Top 5: {feature_imp[:5]}\")\n\n        # Select top features - limit to\
          \ reasonable number\n        max_selected = min(1000, len(feature_cols))\
          \  # Cap at 200 features\n        selected_features = [feat for feat, imp\
          \ in feature_imp[:max_selected] if imp > 0]\n\n        print(f\"Selected\
          \ {len(selected_features)} features using batched LGBM\")\n\n        # Clean\
          \ up\n        del X, y, train_sample, lgb_train, model\n        gc.collect()\n\
          \n        return selected_features\n\n    def _get_dataset_info(dataset_path):\n\
          \        \"\"\"Dataset metadata extraction\"\"\"\n        import pyarrow.parquet\
          \ as pq\n\n        # Use pyarrow to get metadata without loading data\n\
          \        parquet_file = pq.ParquetFile(dataset_path)\n        schema = parquet_file.schema_arrow\n\
          \        total_rows = parquet_file.metadata.num_rows\n\n        # Get column\
          \ names\n        columns = [field.name for field in schema]\n\n        #\
          \ Get approximate dtypes by reading just a small sample\n        sample_df\
          \ = pd.read_parquet(dataset_path).head(100)\n        dtypes = sample_df.dtypes\n\
          \n        return columns, total_rows, dtypes\n\n    def _load_data_chunked(dataset_path,\
          \ chunk_size=10000):\n        \"\"\"Memory efficient data loading\"\"\"\n\
          \        columns, total_rows, dtypes = _get_dataset_info(dataset_path)\n\
          \        print(f\"Dataset info: {total_rows} rows, {len(columns)} columns\"\
          )\n\n        # For very wide datasets, use a different approach\n      \
          \  if len(columns) > 2000:\n            print(f\"Very wide dataset ({len(columns)}\
          \ columns) - using memory-efficient loading\")\n            # Load the entire\
          \ dataset but with immediate optimization\n            print(\"Loading wide\
          \ dataset with immediate memory optimization...\")\n            df = pd.read_parquet(dataset_path)\n\
          \            print(f\"Loaded {df.shape[0]} rows \xD7 {df.shape[1]} columns\"\
          )\n\n            # CRITICAL: Immediately optimize to reduce memory before\
          \ processing\n            print(\"Applying aggressive memory optimization...\"\
          )\n            df = _optimize_dtypes(df)\n\n            # Force garbage\
          \ collection to free up memory\n            gc.collect()\n            _monitor_memory()\n\
          \n            # All columns preserved for comprehensive statistical analysis\n\
          \            print(\"All columns preserved for statistical feature analysis\"\
          )\n\n            return df\n\n        # For normal datasets, use chunked\
          \ loading\n        chunks = []\n        processed_rows = 0\n\n        #\
          \ Since pandas doesn't support chunksize for parquet, we'll load it all\n\
          \        try:\n            df = pd.read_parquet(dataset_path)\n        \
          \    df = _optimize_dtypes(df)\n            print(f\"Loaded and optimized\
          \ dataset: {df.shape}\")\n            _monitor_memory()\n            return\
          \ df\n        except MemoryError:\n            print(\"Memory error - dataset\
          \ too large to load at once\")\n            raise MemoryError(\"Dataset\
          \ too large for available memory\")\n\n    print(\"Starting data preprocessing...\"\
          )\n    _monitor_memory()\n\n    # Load data in chunks to avoid memory issues\n\
          \    df = _load_data_chunked(dataset_input, chunk_size=5000)\n    print(f\"\
          Loaded dataset: {df.shape}\")\n    _monitor_memory()\n\n    # Data validation\n\
          \    _validate_data(df, id_columns, target_col, date_col, train_start, train_end,\
          \ oot_start, oot_end)\n\n    # Basic preprocessing\n    df = _basic_preprocessing(df,\
          \ id_columns, target_col, date_col, ignored_features)\n    _monitor_memory()\n\
          \n    # Train-test split\n    train_df, oot_df = _train_test_split(df, date_col,\
          \ train_start, train_end, oot_start, oot_end)\n\n    # Clean up original\
          \ dataframe to free memory\n    del df\n    gc.collect()\n    _monitor_memory()\n\
          \n    # Comprehensive statistical feature selection\n    print(\"Starting\
          \ comprehensive statistical feature analysis...\")\n    try:\n        selected_features\
          \ = _statistical_feature_analysis(train_df, oot_df, target_col, id_columns,\
          \ date_col)\n    except Exception as e:\n        print(f\"Statistical analysis\
          \ failed: {e}, falling back to LightGBM selection\")\n        selected_features\
          \ = _feature_selection_lgbm_batched(train_df, target_col, id_columns, date_col)\n\
          \n    # Keep only selected features\n    keep_cols = id_columns + [target_col,\
          \ date_col] + selected_features\n    train_df = train_df[keep_cols]\n  \
          \  oot_df = oot_df[keep_cols]\n\n    # Save results\n    train_df.to_parquet(train_output)\n\
          \    oot_df.to_parquet(oot_output)\n\n    # Save feature report\n    report\
          \ = {\n        'selected_features': selected_features,\n        'train_shape':\
          \ train_df.shape,\n        'oot_shape': oot_df.shape\n    }\n    with open(feature_selection_report,\
          \ 'w') as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"Preprocessing\
          \ complete - Train: {train_df.shape}, OOT: {oot_df.shape}\")\n\n    outputs\
          \ = namedtuple('Outputs', ['train_shape', 'oot_shape', 'selected_features',\
          \ 'status'])\n    return outputs(str(train_df.shape), str(oot_df.shape),\
          \ str(selected_features), \"success\")\n\n"
        image: nathangalung246/kubeflow_dummy:latest
        resources:
          cpuLimit: 6.0
          cpuRequest: 2.0
          memoryLimit: 6.442450944
          memoryRequest: 4.294967296
          resourceCpuLimit: '6'
          resourceCpuRequest: '2'
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 4Gi
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    train_input: InputPath(),\n    oot_input: InputPath(),\n\
          \    feature_selection_report: InputPath(),\n    target_col: str,\n    id_columns:\
          \ list,\n    optimization_metric: str,\n    project_name: str,\n    max_models:\
          \ int,\n    max_runtime_secs: int,\n    automl_seed: int,\n    use_cross_validation:\
          \ str,\n    model_output: OutputPath(),\n    feature_list_output: OutputPath()\n\
          ) -> NamedTuple('Outputs', [\n    ('train_score', float),\n    ('oot_score',\
          \ float),\n    ('final_features', str),\n    ('status', str),\n    ('mlflow_run_id',\
          \ str),\n    ('model_name', str),\n    ('model_version', str)\n]):\n   \
          \ import pandas as pd\n    import numpy as np\n    import json\n    import\
          \ tempfile\n    import shutil\n    import os\n    from collections import\
          \ namedtuple\n    import h2o\n    from h2o.automl import H2OAutoML\n   \
          \ import mlflow\n    import mlflow.h2o\n\n    def _train_h2o_model(X_train,\
          \ y_train, X_oot, y_oot, optimization_metric, max_models, max_runtime_secs,\
          \ automl_seed, use_cross_validation):\n        \"\"\"H2O AutoML training\"\
          \"\"\n        # Initialize H2O for wide datasets\n        h2o.init(max_mem_size=\"\
          10G\", nthreads=8, strict_version_check=False)\n\n        try:\n       \
          \     # Wide dataset handling\n            num_features = len(X_train.columns)\n\
          \            num_rows = len(X_train)\n\n            print(f\"Dataset dimensions:\
          \ {num_rows} rows \xD7 {num_features} columns\")\n            print(f\"\
          Memory concern: Wide dataset with {num_features} features\")\n\n       \
          \     # Use preprocessed features\n            X_train_sample = X_train\n\
          \            y_train_sample = y_train\n            X_oot_sample = X_oot\n\
          \            y_oot_sample = y_oot\n\n            print(f\"Using full dataset:\
          \ {len(X_train_sample)} train rows, {len(X_oot_sample)} OOT rows\")\n  \
          \          print(f\"Feature count after preprocessing: {len(X_train_sample.columns)}\
          \ features\")\n\n            # Convert to H2O frames\n            print(\"\
          Converting training data to H2O frame...\")\n            train_data = pd.concat([X_train_sample,\
          \ y_train_sample], axis=1)\n            train_h2o = h2o.H2OFrame(train_data)\n\
          \            del train_data, X_train_sample, y_train_sample\n\n        \
          \    print(\"Converting OOT data to H2O frame...\")\n            oot_data\
          \ = pd.concat([X_oot_sample, y_oot_sample], axis=1)\n            oot_h2o\
          \ = h2o.H2OFrame(oot_data)\n            del oot_data, X_oot_sample, y_oot_sample\n\
          \n            train_h2o[y_train.name] = train_h2o[y_train.name].asfactor()\n\
          \            oot_h2o[y_oot.name] = oot_h2o[y_oot.name].asfactor()\n\n  \
          \          # AutoML configuration\n            aml = H2OAutoML(\n      \
          \          max_models=max_models,\n                max_runtime_secs=max_runtime_secs,\n\
          \                sort_metric='AUCPR' if optimization_metric.upper() == 'AUCPR'\
          \ else 'AUC',\n                seed=automl_seed,\n                nfolds=5\
          \ if use_cross_validation.upper() == \"YES\" else 0,\n                #\
          \ Performance parameters\n                max_runtime_secs_per_model=max_runtime_secs\
          \ // max_models if max_models > 0 else 300,\n                stopping_metric='AUCPR'\
          \ if optimization_metric.upper() == 'AUCPR' else 'AUC',\n              \
          \  stopping_tolerance=0.005,\n                stopping_rounds=10,\n    \
          \            balance_classes=True,\n                class_sampling_factors=None,\n\
          \                max_after_balance_size=2.0\n            )\n\n         \
          \   aml.train(x=list(X_train.columns), y=y_train.name, training_frame=train_h2o)\n\
          \n            # Get best model\n            best_model = aml.leader\n\n\
          \            # Calculate performance\n            train_perf = best_model.model_performance(train_h2o)\n\
          \            oot_perf = best_model.model_performance(oot_h2o)\n\n      \
          \      if optimization_metric.upper() == 'AUCPR':\n                train_score\
          \ = train_perf.aucpr()\n                oot_score = oot_perf.aucpr()\n \
          \           else:\n                train_score = train_perf.auc()\n    \
          \            oot_score = oot_perf.auc()\n\n            return best_model,\
          \ float(train_score), float(oot_score)\n\n        except Exception as e:\n\
          \            h2o.cluster().shutdown()\n            raise e\n\n    def _setup_mlflow():\n\
          \        \"\"\"MLflow connection setup\"\"\"\n        mlflow_endpoints =\
          \ [\n            \"http://mlflow-service.kubeflow.svc.cluster.local:5000\"\
          ,\n            \"http://mlflow-service.kubeflow:5000\", \n            \"\
          http://10.96.235.221:5000\",\n            \"http://localhost:5000\"\n  \
          \      ]\n\n        for endpoint in mlflow_endpoints:\n            try:\n\
          \                mlflow.set_tracking_uri(endpoint)\n                # Connection\
          \ test\n                mlflow.get_experiment_by_name(\"Default\")\n   \
          \             print(f\"MLflow connected successfully to: {endpoint}\")\n\
          \                break\n            except Exception as e:\n           \
          \     print(f\"Failed to connect to {endpoint}: {e}\")\n               \
          \ continue\n        else:\n            print(\"Warning: Could not connect\
          \ to MLflow, using local tracking\")\n            mlflow.set_tracking_uri(\"\
          file:///tmp/mlruns\")\n\n    # Setup MLflow\n    _setup_mlflow()\n\n   \
          \ def _load_data_with_memory_check(file_path, max_memory_mb=8000):\n   \
          \     \"\"\"Memory-aware data loading\"\"\"\n        import psutil\n   \
          \     process = psutil.Process()\n        initial_memory = process.memory_info().rss\
          \ / 1024 / 1024\n\n        # File size check\n        import os\n      \
          \  file_size_mb = os.path.getsize(file_path) / 1024 / 1024\n        print(f\"\
          Loading file: {file_size_mb:.1f}MB, Current memory: {initial_memory:.1f}MB\"\
          )\n\n        if initial_memory + file_size_mb > max_memory_mb:\n       \
          \     print(f\"Memory limit would be exceeded, loading in chunks\")\n  \
          \          # Chunked loading\n            chunks = []\n            for chunk\
          \ in pd.read_parquet(file_path, chunksize=10000):\n                chunks.append(chunk)\n\
          \                current_memory = process.memory_info().rss / 1024 / 1024\n\
          \                if current_memory > max_memory_mb:\n                  \
          \  print(f\"Memory limit reached at {current_memory:.1f}MB, processing {len(chunks)}\
          \ chunks\")\n                    break\n                if current_memory\
          \ > max_memory_mb * 1.2:  # Emergency stop\n                    print(f\"\
          EMERGENCY: Memory usage {current_memory:.1f}MB critical - stopping immediately\"\
          )\n                    raise MemoryError(f\"Memory usage {current_memory:.1f}MB\
          \ exceeds emergency threshold\")\n            df = pd.concat(chunks, ignore_index=True)\n\
          \            del chunks\n            import gc\n            gc.collect()\n\
          \        else:\n            df = pd.read_parquet(file_path)\n\n        final_memory\
          \ = process.memory_info().rss / 1024 / 1024\n        print(f\"Data loaded:\
          \ {df.shape}, Memory: {final_memory:.1f}MB\")\n        return df\n\n   \
          \ # Load datasets with memory monitoring\n    print(\"Loading training data...\"\
          )\n    train_df = _load_data_with_memory_check(train_input)\n    print(\"\
          Loading OOT data...\")\n    oot_df = _load_data_with_memory_check(oot_input)\n\
          \n    with open(feature_selection_report, 'r') as f:\n        feature_report\
          \ = json.load(f)\n\n    feature_cols = feature_report['selected_features']\n\
          \n    # Prepare data\n    X_train = train_df[feature_cols]\n    y_train\
          \ = train_df[target_col]\n    X_oot = oot_df[feature_cols]\n    y_oot =\
          \ oot_df[target_col]\n\n    print(f\"Training model with {len(feature_cols)}\
          \ features\")\n    print(f\"Train samples: {len(X_train)}, OOT samples:\
          \ {len(X_oot)}\")\n\n    # Start MLflow run\n    experiment_name = f\"{project_name}-automl-experiment\"\
          \n    try:\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n\
          \        if experiment is None:\n            experiment_id = mlflow.create_experiment(experiment_name)\n\
          \        else:\n            experiment_id = experiment.experiment_id\n \
          \   except:\n        experiment_id = \"0\"  # Default experiment\n\n   \
          \ with mlflow.start_run(experiment_id=experiment_id, run_name=f\"{project_name}-h2o-automl-run\"\
          ) as run:\n        run_id = run.info.run_id\n        print(f\"MLflow run\
          \ started: {run_id}\")\n\n        # Log parameters\n        mlflow.log_param(\"\
          optimization_metric\", optimization_metric)\n        mlflow.log_param(\"\
          max_models\", max_models)\n        mlflow.log_param(\"max_runtime_secs\"\
          , max_runtime_secs)\n        mlflow.log_param(\"automl_seed\", automl_seed)\n\
          \        mlflow.log_param(\"use_cross_validation\", use_cross_validation)\n\
          \        mlflow.log_param(\"num_features\", len(feature_cols))\n       \
          \ mlflow.log_param(\"train_samples\", len(X_train))\n        mlflow.log_param(\"\
          oot_samples\", len(X_oot))\n\n        # Train model\n        model, train_score,\
          \ oot_score = _train_h2o_model(\n            X_train, y_train, X_oot, y_oot,\
          \ optimization_metric,\n            max_models, max_runtime_secs, automl_seed,\
          \ use_cross_validation\n        )\n\n        # Log metrics\n        mlflow.log_metric(f\"\
          train_{optimization_metric.lower()}\", train_score)\n        mlflow.log_metric(f\"\
          oot_{optimization_metric.lower()}\", oot_score)\n        mlflow.log_metric(\"\
          train_oot_diff\", abs(train_score - oot_score))\n\n        # Save model\
          \ using H2O's native save method\n        model_name = f\"h2o-automl-{optimization_metric.lower()}-model\"\
          \n\n        # Save H2O model - it can be a single file or directory depending\
          \ on model type\n        with tempfile.TemporaryDirectory() as temp_dir:\n\
          \            # H2O reinitialization\n            h2o.init()\n          \
          \  try:\n                model_path = h2o.save_model(model=model, path=temp_dir,\
          \ force=True)\n                print(f\"H2O model saved to temp path: {model_path}\"\
          )\n\n                # Model path handling\n                if os.path.isdir(model_path):\n\
          \                    # Directory copy\n                    shutil.copytree(model_path,\
          \ model_output, dirs_exist_ok=True)\n                else:\n           \
          \         # File copy\n                    shutil.copy2(model_path, model_output)\n\
          \n                print(f\"H2O model copied to: {model_output}\")\n\n  \
          \              # Log model as artifact to MLflow\n                try:\n\
          \                    mlflow.log_artifact(model_path, \"model\")\n      \
          \              print(\"Model artifact logged to MLflow\")\n            \
          \    except Exception as e:\n                    print(f\"Warning: Could\
          \ not log model artifact to MLflow: {e}\")\n            finally:\n     \
          \           h2o.cluster().shutdown()\n\n        # Save feature list\n  \
          \      feature_data = {\n            \"final_features\": feature_cols,\n\
          \            \"model_type\": \"h2o_automl\",\n            \"optimization_metric\"\
          : optimization_metric,\n            \"train_score\": float(train_score),\n\
          \            \"oot_score\": float(oot_score)\n        }\n        with open(feature_list_output,\
          \ 'w') as f:\n            json.dump(feature_data, f, indent=2)\n\n     \
          \   # Log feature list as artifact\n        try:\n            mlflow.log_artifact(feature_list_output,\
          \ \"features\")\n            print(\"Feature list logged to MLflow\")\n\
          \        except Exception as e:\n            print(f\"Warning: Could not\
          \ log feature list to MLflow: {e}\")\n\n        # Log additional metadata\n\
          \        mlflow.set_tag(\"model_type\", \"h2o_automl\")\n        mlflow.set_tag(\"\
          project_name\", project_name)\n        mlflow.set_tag(\"kubeflow_component\"\
          , \"train_model\")\n\n        print(f\"Model training complete - {optimization_metric}:\
          \ Train={train_score:.4f}, OOT={oot_score:.4f}\")\n        print(f\"MLflow\
          \ run ID: {run_id}\")\n\n        # Register model in MLflow Model Registry\
          \ with auto-increment versioning\n        def _register_model_with_auto_increment(model_uri,\
          \ base_model_name, max_attempts=10):\n            \"\"\"Auto-increment model\
          \ registration\"\"\"\n            current_model_name = base_model_name\n\
          \            attempt = 1\n\n            while attempt <= max_attempts:\n\
          \                try:\n                    # Model registration attempt\n\
          \                    model_version = mlflow.register_model(\n          \
          \              model_uri=model_uri,\n                        name=current_model_name\n\
          \                    )\n                    print(f\"Model registered successfully:\
          \ {current_model_name} v{model_version.version}\")\n                   \
          \ return current_model_name, model_version.version\n\n                except\
          \ Exception as e:\n                    error_msg = str(e).lower()\n\n  \
          \                  # Conflict detection\n                    if \"already\
          \ exists\" in error_msg or \"conflict\" in error_msg:\n                \
          \        # Versioned name fallback\n                        current_model_name\
          \ = f\"{base_model_name}-v{attempt}\"\n                        print(f\"\
          Model name conflict, trying: {current_model_name}\")\n                 \
          \       attempt += 1\n                    else:\n                      \
          \  # Error handling\n                        print(f\"MLflow registration\
          \ error: {e}\")\n\n                        # Check if it's a model registry\
          \ unavailable error\n                        if \"404\" in str(e) or \"\
          not found\" in error_msg:\n                            print(\"MLflow Model\
          \ Registry not available, using run-based model tracking\")\n          \
          \                  return base_model_name, \"1\"\n\n                   \
          \     try:\n                            # Existing model check\n       \
          \                     client = mlflow.MlflowClient()\n                 \
          \           registered_model = client.get_registered_model(base_model_name)\n\
          \                            latest_version = registered_model.latest_versions[0].version\
          \ if registered_model.latest_versions else \"1\"\n                     \
          \       print(f\"Using existing model: {base_model_name} (latest version:\
          \ {latest_version})\")\n                            return base_model_name,\
          \ latest_version\n                        except:\n                    \
          \        print(\"Could not access existing model info, using default\")\n\
          \                            return base_model_name, \"1\"\n\n         \
          \   # Default fallback\n            print(f\"Could not register model after\
          \ {max_attempts} attempts\")\n            return base_model_name, \"1\"\n\
          \n        try:\n            final_model_name, model_version_number = _register_model_with_auto_increment(\n\
          \                f\"runs:/{run_id}/model\", \n                model_name\n\
          \            )\n            model_name = final_model_name  # Update model_name\
          \ to the final registered name\n            print(f\"Final model registration:\
          \ {model_name} v{model_version_number}\")\n        except Exception as e:\n\
          \            print(f\"Warning: Could not register model in MLflow Registry:\
          \ {e}\")\n            model_version_number = \"1\"\n\n        outputs =\
          \ namedtuple('Outputs', ['train_score', 'oot_score', 'final_features', 'status',\
          \ 'mlflow_run_id', 'model_name', 'model_version'])\n        return outputs(float(train_score),\
          \ float(oot_score), str(feature_cols), \"success\", run_id, model_name,\
          \ model_version_number)\n\n"
        env:
        - name: EXPLAINABLE_MODEL
          value: 'YES'
        - name: MAX_MODELS
          value: '2'
        - name: MAX_RUNTIME_SECS
          value: '120'
        - name: USE_CROSS_VALIDATION
          value: 'YES'
        - name: AUTOML_SEED
          value: '42'
        image: nathangalung246/kubeflow_dummy:latest
        resources:
          cpuLimit: 8.0
          cpuRequest: 4.0
          memoryLimit: 12.884901888
          memoryRequest: 6.442450944
          resourceCpuLimit: '8'
          resourceCpuRequest: '4'
          resourceMemoryLimit: 12Gi
          resourceMemoryRequest: 6Gi
pipelineInfo:
  description: DummyDataModel ML Pipeline with Modular Components
  name: dummydata
root:
  dag:
    tasks:
      collect-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-collect-data
        inputs:
          parameters:
            minio_access_key:
              runtimeValue:
                constant: minio
            minio_bucket:
              runtimeValue:
                constant: mlpipeline
            minio_dataset_object:
              runtimeValue:
                constant: dataset.parquet
            minio_endpoint:
              runtimeValue:
                constant: minio-service.kubeflow:9000
            minio_secret_key:
              runtimeValue:
                constant: minio123
        taskInfo:
          name: collect-data
      deploy-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-deploy-model
        dependentTasks:
        - evaluate-model
        - train-model
        inputs:
          artifacts:
            feature_list_path:
              taskOutputArtifact:
                outputArtifactKey: feature_list_output
                producerTask: train-model
            model_path:
              taskOutputArtifact:
                outputArtifactKey: model_output
                producerTask: train-model
          parameters:
            evaluation_summary:
              taskOutputParameter:
                outputParameterKey: evaluation_summary
                producerTask: evaluate-model
            kserve_deployment_mode:
              runtimeValue:
                constant: Serverless
            kserve_namespace:
              runtimeValue:
                constant: dummypipeline
            mlflow_run_id:
              taskOutputParameter:
                outputParameterKey: mlflow_run_id
                producerTask: train-model
            model_name:
              taskOutputParameter:
                outputParameterKey: model_name
                producerTask: train-model
            model_version:
              taskOutputParameter:
                outputParameterKey: model_version
                producerTask: train-model
        taskInfo:
          name: deploy-model
      evaluate-model:
        cachingOptions: {}
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - preprocess-data
        - train-model
        inputs:
          artifacts:
            model_input:
              taskOutputArtifact:
                outputArtifactKey: model_output
                producerTask: train-model
            oot_input:
              taskOutputArtifact:
                outputArtifactKey: oot_output
                producerTask: preprocess-data
            train_input:
              taskOutputArtifact:
                outputArtifactKey: train_output
                producerTask: preprocess-data
          parameters:
            oot_score:
              taskOutputParameter:
                outputParameterKey: oot_score
                producerTask: train-model
            optimization_metric:
              runtimeValue:
                constant: AUCPR
            train_score:
              taskOutputParameter:
                outputParameterKey: train_score
                producerTask: train-model
        taskInfo:
          name: evaluate-model
      preprocess-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-data
        dependentTasks:
        - collect-data
        inputs:
          artifacts:
            dataset_input:
              taskOutputArtifact:
                outputArtifactKey: dataset_output
                producerTask: collect-data
          parameters:
            date_col:
              runtimeValue:
                constant: partition_date
            id_columns:
              runtimeValue:
                constant:
                - risk_id
            ignored_features:
              runtimeValue:
                constant: []
            oot_end:
              runtimeValue:
                constant: '2024-09-30'
            oot_start:
              runtimeValue:
                constant: '2024-09-01'
            target_col:
              runtimeValue:
                constant: label
            train_end:
              runtimeValue:
                constant: '2024-08-31'
            train_start:
              runtimeValue:
                constant: '2024-01-01'
        taskInfo:
          name: preprocess-data
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - preprocess-data
        inputs:
          artifacts:
            feature_selection_report:
              taskOutputArtifact:
                outputArtifactKey: feature_selection_report
                producerTask: preprocess-data
            oot_input:
              taskOutputArtifact:
                outputArtifactKey: oot_output
                producerTask: preprocess-data
            train_input:
              taskOutputArtifact:
                outputArtifactKey: train_output
                producerTask: preprocess-data
          parameters:
            automl_seed:
              runtimeValue:
                constant: 42.0
            id_columns:
              runtimeValue:
                constant:
                - risk_id
            max_models:
              runtimeValue:
                constant: 2.0
            max_runtime_secs:
              runtimeValue:
                constant: 120.0
            optimization_metric:
              runtimeValue:
                constant: AUCPR
            project_name:
              runtimeValue:
                constant: DummyData
            target_col:
              runtimeValue:
                constant: label
            use_cross_validation:
              runtimeValue:
                constant: 'YES'
        taskInfo:
          name: train-model
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
